{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,precision_recall_fscore_support\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def save_csv(df,csvpath):\n",
    "    terms_mini = df[['TERMTYPE']+['TERM_VECTOR{}'.format(y) for y in range(0,10)]]\n",
    "    terms_mini_business = terms_mini[terms_mini['TERMTYPE'] =='business']\n",
    "    terms_mini_nonbusiness = terms_mini[terms_mini['TERMTYPE'] !='business'].sample(n=terms_mini_business.shape[0])\n",
    "    terms_mini = pd.concat([terms_mini_business,terms_mini_nonbusiness])\n",
    "    terms_mini = terms_mini.iloc[np.random.permutation(len(terms_mini))]\n",
    "    terms_mini.to_csv(csvpath,index=False,sep=',')\n",
    "    return terms_mini"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "raw_terms = pd.read_csv('unique-ngram.csv', sep='\\t').drop(['Unnamed: 0'],axis=1)\n",
    "raw_terms2 = raw_terms.sample(100000)\n",
    "save_csv(raw_terms2, 'all_ngram_ML_less.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_test_validation(filename):\n",
    "    raw_terms = pickle.load(open(filename,'rb')).drop(['TERM_VECTOR'], axis=1)\n",
    "    terms = raw_terms\n",
    "    #terms = raw_terms[raw_terms['OCCURENCES']>2]\n",
    "    terms.drop(['NGRAM','OCCURENCES'], axis=1, inplace=True)\n",
    "    bterm = terms[terms['BUSINESS']==1]['TERM']\n",
    "    nbterm = terms[terms['BUSINESS']==0]['TERM']\n",
    "    clean_term = terms[~((terms['BUSINESS']==0) & (terms['TERM'].isin(bterm))) & ~((terms['BUSINESS']==1) & (terms['TERM'].isin(nbterm)))]\n",
    "    \n",
    "    print(clean_term.shape)\n",
    "    print(clean_term.groupby('BUSINESS')['BUSINESS'].count())\n",
    "    \n",
    "    business_terms = clean_term[clean_term['BUSINESS']==1]\n",
    "    nonbusiness_terms = clean_term[clean_term['BUSINESS']==0]\n",
    "    \n",
    "    business_train, business_test_validation = train_test_split(business_terms, test_size = 0.3)\n",
    "    business_test, business_validation = train_test_split(business_test_validation, test_size = 1/3)\n",
    "    nonbusiness_train, nonbusiness_test_validation = train_test_split(nonbusiness_terms, test_size = 0.3)\n",
    "    nonbusiness_test, nonbusiness_validation = train_test_split(nonbusiness_test_validation, test_size = 1/3)\n",
    "    del business_test_validation\n",
    "    del nonbusiness_test_validation\n",
    "    \n",
    "    #nonbusiness_train = nonbusiness_train.sample(n=len(business_train))\n",
    "    #nonbusiness_validation = nonbusiness_validation.sample(n=len(business_validation))\n",
    "    #nonbusiness_test = nonbusiness_test.sample(n=len(business_test))\n",
    "\n",
    "    train_labels = pd.concat([business_train,nonbusiness_train]).drop(['BUSINESS'],axis=1)\n",
    "    train_labels['SET'] = 'TRAIN'\n",
    "    validation_labels = pd.concat([business_validation,nonbusiness_validation]).drop(['BUSINESS'],axis=1)\n",
    "    validation_labels['SET'] = 'VALIDATION'\n",
    "    test_labels = pd.concat([business_test,nonbusiness_test]).drop(['BUSINESS'],axis=1)\n",
    "    test_labels['SET'] = 'TEST'\n",
    "    \n",
    "    train_validation_test = pd.concat([train_labels, validation_labels, test_labels])\n",
    "    \n",
    "    return train_validation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features_labels(filename, vectortype, train_validation_test):\n",
    "    raw_terms = pickle.load(open(filename,'rb')).rename(columns={'TERM_VECTOR':'FEATURE_VECTOR'})\n",
    "    bterm = raw_terms[raw_terms['BUSINESS']==1]['TERM']\n",
    "    nbterm = raw_terms[raw_terms['BUSINESS']==0]['TERM']\n",
    "    raw_terms = raw_terms[~((raw_terms['BUSINESS']==0) & (raw_terms['TERM'].isin(bterm))) & ~((raw_terms['BUSINESS']==1) & (raw_terms['TERM'].isin(nbterm)))]\n",
    "    \n",
    "    if vectortype == 'tfidf':\n",
    "        terms_count = raw_terms.groupby('TERM')['TERM'].count()\n",
    "        terms =  raw_terms[raw_terms['TERM'].map(terms_count)>4]\n",
    "        #terms['FEATURE_VECTOR'] = terms.apply(lambda x: [x['TERM_VECTOR{}'.format(y)] for y in range(0,10)],axis=1)\n",
    "        terms.drop(['NGRAM','ROWNUM'], axis=1, inplace=True)\n",
    "    else:\n",
    "        terms = raw_terms\n",
    "        #terms = raw_terms[raw_terms['OCCURENCES']>2]\n",
    "        #terms['FEATURE_VECTOR'] = terms.apply(lambda x: [x['TERM_VECTOR{}'.format(y)] for y in range(0,10)],axis=1)\n",
    "        #terms.drop(['TERM_VECTOR{}'.format(y) for y in range(0,10)]+['NGRAM','ROWNUM','TERMTYPE'], axis=1, inplace=True)\n",
    "        terms.drop(['NGRAM','OCCURENCES'], axis=1, inplace=True)\n",
    "    \n",
    "    terms = terms.merge(train_validation_test, 'left', 'TERM')\n",
    "    train_dataset =  terms[terms['SET']=='TRAIN'].drop(['SET'],axis=1)\n",
    "    validation_dataset =  terms[terms['SET']=='VALIDATION'].drop(['SET'],axis=1)\n",
    "    test_dataset =  terms[terms['SET']=='TEST'].drop(['SET'],axis=1)\n",
    "\n",
    "    train_dataset = train_dataset.iloc[np.random.permutation(len(train_dataset))]\n",
    "    validation_dataset = validation_dataset.iloc[np.random.permutation(len(validation_dataset))]\n",
    "    test_dataset = test_dataset.iloc[np.random.permutation(len(test_dataset))]\n",
    "    \n",
    "    print(\"Train-\")\n",
    "    print(train_dataset.groupby('BUSINESS')['BUSINESS'].count())\n",
    "    print(\"Validation-\")\n",
    "    print(validation_dataset.groupby('BUSINESS')['BUSINESS'].count())\n",
    "    print(\"Test-\")\n",
    "    print(test_dataset.groupby('BUSINESS')['BUSINESS'].count())\n",
    "    \n",
    "    train_features = np.vstack(train_dataset['FEATURE_VECTOR'].map(lambda x : np.array([0.000001 if y==0 else y for y in x]) ).values)\n",
    "    train_labels = np.vstack(train_dataset['BUSINESS'].values)\n",
    "    validation_features = np.vstack(validation_dataset['FEATURE_VECTOR'].map(lambda x : np.array([0.000001 if y==0 else y for y in x]) ).values)\n",
    "    validation_labels = np.vstack(validation_dataset['BUSINESS'].values)\n",
    "    test_features = np.vstack(test_dataset['FEATURE_VECTOR'].map(lambda x : np.array([0.000001 if y==0 else y for y in x]) ).values)\n",
    "    test_labels = np.vstack(test_dataset['BUSINESS'].values)\n",
    "    \n",
    "    return train_features, train_labels, validation_features, validation_labels,  test_features, test_labels\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_features, train_labels, validation_features, validation_labels,  test_features, test_labels = create_features_labels('ngram_10krows_ml_kn.pickle','ml')\n",
    "train_features, train_labels, validation_features, validation_labels,  test_features, test_labels = create_features_labels('ngram_10krows_word2vec.pickle','word2vec')\n",
    "train_features, train_labels, validation_features, validation_labels,  test_features, test_labels = create_features_labels('ngram_10krows_tfidf.pickle','tfidf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "somelist = []\n",
    "for termtype in ['business','nonbusiness']:\n",
    "    for ngram in range(2,11):\n",
    "        temp_df = terms[ (terms['NGRAM']==ngram) & (terms['TERMTYPE']==termtype) ].sample(500)\n",
    "        somelist.append(temp_df)\n",
    "new_df_balanced = pd.concat(somelist)\n",
    "save_csv(new_df_balanced, 'all_ngram_500.csv')\n",
    "\n",
    "specificgram_terms =  raw_terms[raw_terms['NGRAM']==3 ]\n",
    "save_csv(specificgram_terms,'10gram.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_dataset['BUSINESS'] = train_dataset['BUSINESS'].map(lambda x: [0,1] if x==1 else [1,0])\n",
    "test_dataset['BUSINESS'] = test_dataset['BUSINESS'].map(lambda x: [0,1] if x==1 else [1,0])\n",
    "train_features = np.vstack(train_dataset['FEATURE_VECTOR'].map(lambda x : np.array([0.000001 if y==0 else y for y in x]) ).values)\n",
    "train_labels = np.vstack(train_dataset['BUSINESS'].map(np.array).values)\n",
    "test_features = np.vstack(test_dataset['FEATURE_VECTOR'].map(lambda x : np.array([0.000001 if y==0 else y for y in x]) ).values)\n",
    "test_labels = np.vstack(test_dataset['BUSINESS'].map(np.array).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "def create_architecture(n_input, layers):\n",
    "\n",
    "    hidden_features = {\n",
    "        10:[7,5,3,2],\n",
    "        1000:[666,444,300,200]\n",
    "    }\n",
    "\n",
    "    n_hidden_1 = hidden_features[n_input][0] # 1st layer number of features\n",
    "    n_hidden_2 = hidden_features[n_input][1] # 2nd layer number of features\n",
    "    n_hidden_3 = hidden_features[n_input][2] # 3rd layer number of features\n",
    "    n_hidden_4 = hidden_features[n_input][3] # 3rd layer number of features\n",
    "    n_classes = 1 # total classes (binary)\n",
    "\n",
    "    # Create model\n",
    "    def multilayer_perceptron(layers, x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        layer_4 = tf.nn.relu(layer_4)\n",
    "            \n",
    "        #Output layer with linear activation\n",
    "        out_layer = {\n",
    "            1: tf.matmul(layer_1, weights['out1']) + biases['out'],\n",
    "            2: tf.matmul(layer_2, weights['out2']) + biases['out'],\n",
    "            3: tf.matmul(layer_3, weights['out3']) + biases['out'],\n",
    "            4: tf.matmul(layer_4, weights['out4']) + biases['out']\n",
    "                     }\n",
    "        return out_layer[layers]\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input,n_hidden_1],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_input+\n",
    "                                                             n_hidden_1+1)),\n",
    "                                           name=\"h1\")),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_1+\n",
    "                                                             n_hidden_2+1)),\n",
    "                                           name=\"h2\")),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_2,n_hidden_3],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_2+\n",
    "                                                             n_hidden_3+1)),\n",
    "                                           name=\"h3\")),\n",
    "        'h4': tf.Variable(tf.random_normal([n_hidden_3,n_hidden_4],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_3+\n",
    "                                                             n_hidden_4+1)),\n",
    "                                           name=\"h4\")),\n",
    "        'out1': tf.Variable(tf.random_normal([n_hidden_1,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_1+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\")),\n",
    "        'out2': tf.Variable(tf.random_normal([n_hidden_2,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_2+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\")),\n",
    "        'out3': tf.Variable(tf.random_normal([n_hidden_3,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_3+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\")),\n",
    "        'out4': tf.Variable(tf.random_normal([n_hidden_4,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_4+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\"))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_1+1)),\n",
    "                                        name=\"b1\")),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_2+1)),\n",
    "                                        name=\"b2\")),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_3+1)),\n",
    "                                        name=\"b3\")),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_4],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_4+1)),\n",
    "                                        name=\"b4\")),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_classes+1)),\n",
    "                                        name=\"biasout\"))\n",
    "    }\n",
    "    \n",
    "    # Parameters\n",
    "    learning_rate = 0.001 # the alpha\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(layers, x, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    #cost = tf.nn.l2_loss(pred-y, name=\"squared_error_cost\")\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    predict_op = tf.nn.sigmoid(pred)\n",
    "    \n",
    "    return x, y, cost, optimizer, predict_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"dce18f49-3903-48ca-9208-35b38bf8fc1c\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#dce18f49-3903-48ca-9208-35b38bf8fc1c\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"dce18f49-3903-48ca-9208-35b38bf8fc1c\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'dce18f49-3903-48ca-9208-35b38bf8fc1c' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#dce18f49-3903-48ca-9208-35b38bf8fc1c\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#dce18f49-3903-48ca-9208-35b38bf8fc1c\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"plotdiv\" id=\"8106c150-6080-427d-90d7-8a22ef54c27a\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = \"\";\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        Bokeh.$(\"#8106c150-6080-427d-90d7-8a22ef54c27a\").text(\"BokehJS successfully loaded.\");\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }if ((window.Jupyter !== undefined) && Jupyter.notebook.kernel) {\n",
       "      comm_manager = Jupyter.notebook.kernel.comm_manager\n",
       "      comm_manager.register_target(\"313e8ccd-0dff-4252-bc4f-bae5d0e21d76\", function () {});\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"8106c150-6080-427d-90d7-8a22ef54c27a\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid '8106c150-6080-427d-90d7-8a22ef54c27a' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        Bokeh.$(function() {\n",
       "            var docs_json = {\"d6286289-5738-44d2-8e96-76b5f816ee1d\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"501bee41-29c9-420d-b384-6b210c5022c2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"ea72b81e-b2c8-469f-ae87-7f05054e5bbd\",\"type\":\"HelpTool\"},{\"attributes\":{\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c35f63ce-58a6-4325-99cf-8d6cbe6e18f8\",\"type\":\"BasicTicker\"}},\"id\":\"1f5ef60e-e008-4a55-bc1e-3b5bfac0caea\",\"type\":\"Grid\"},{\"attributes\":{\"overlay\":{\"id\":\"24d9d55a-075e-4bab-ae05-96e1dc462461\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"7e646d43-718a-403b-b6e2-249d40c1c03f\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"items\":[{\"id\":\"2d0aa1df-0f1b-4cf2-a156-b0bad581cf10\",\"type\":\"LegendItem\"}],\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"42e96c63-20a1-4a74-a8ad-2bf488316f22\",\"type\":\"Legend\"},{\"attributes\":{\"callback\":null},\"id\":\"9966448e-1fbf-457c-819a-d9c07445858c\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"7d019933-0e1e-4feb-85f1-4eaf80fb01ec\",\"type\":\"ToolEvents\"},{\"attributes\":{},\"id\":\"4fb71642-4700-453e-a274-e65dd33733ce\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"6991bc4e-7fdd-44d5-8835-62aa4a0760bb\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1866d002-afbe-4382-a6d8-9a5f4b869e32\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"987a3d0e-177e-4ba6-8703-932d940a1adf\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"9785489e-3e7b-4d5f-9297-09cbec013dc8\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"2e8379f6-25ec-4d35-99e5-156466f75922\",\"type\":\"BasicTicker\"},{\"attributes\":{\"below\":[{\"id\":\"b658014b-6a7e-4854-bb14-e7580a3cf8d0\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"0064df07-3c12-45d3-a54c-7e9bd633cce1\",\"type\":\"LinearAxis\"}],\"plot_height\":300,\"renderers\":[{\"id\":\"b658014b-6a7e-4854-bb14-e7580a3cf8d0\",\"type\":\"LinearAxis\"},{\"id\":\"f5e0752d-2953-4dc6-ac08-6a23e858bc89\",\"type\":\"Grid\"},{\"id\":\"0064df07-3c12-45d3-a54c-7e9bd633cce1\",\"type\":\"LinearAxis\"},{\"id\":\"4778d811-0e2f-4ff1-8bb4-169cac765236\",\"type\":\"Grid\"},{\"id\":\"5a0d3e74-a7b0-4966-8ef1-a8a44ade9a42\",\"type\":\"BoxAnnotation\"},{\"id\":\"b19eab68-fa5e-4f25-83ab-d442a3ecab18\",\"type\":\"Legend\"},{\"id\":\"9785489e-3e7b-4d5f-9297-09cbec013dc8\",\"type\":\"GlyphRenderer\"},{\"id\":\"952dc3b9-86cc-4cea-8687-d3bb63f85ef9\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"46859bdc-e27e-4a14-aa8a-8a917b4d69bf\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"dcd7a809-9204-447a-9c0b-511b321d60c0\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"25bf169e-c15a-4ffa-978e-9adff52abd89\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"f5634098-29bf-4035-83da-f98c358d6780\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"9966448e-1fbf-457c-819a-d9c07445858c\",\"type\":\"DataRange1d\"}},\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"children\":[{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"80f08ba2-1cba-4835-a6c9-0ae77bf55848\",\"type\":\"Row\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"835a2895-4e3e-44b8-92f0-8b8a6443669a\",\"type\":\"Line\"},{\"attributes\":{\"children\":[{\"id\":\"92629c4a-4e5d-4c73-8b22-29b2859e9be6\",\"type\":\"ToolbarBox\"},{\"id\":\"dab1febb-5625-440c-ade2-5cf1e610c4a4\",\"type\":\"Column\"}]},\"id\":\"0a43d5e9-2e0b-46cb-8ff2-6754ef2c97ae\",\"type\":\"Column\"},{\"attributes\":{\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"2523dcc8-e9a5-456b-a89f-26703d26f2c4\",\"type\":\"PanTool\"},{\"attributes\":{\"callback\":null},\"id\":\"835b2c52-f1c9-4a26-aa17-1b355c79f93c\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"269feea5-7165-40e8-b3d9-3cfc5ad8627a\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"formatter\":{\"id\":\"a8d51af1-c6a7-4fff-a20c-50008935e103\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2e8379f6-25ec-4d35-99e5-156466f75922\",\"type\":\"BasicTicker\"}},\"id\":\"b658014b-6a7e-4854-bb14-e7580a3cf8d0\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"e1c096d5-8824-4ee9-98c0-65ebc1112062\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_color\":{\"value\":\"navy\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"e26f4acc-ca40-46cd-84cf-55817b29e790\",\"type\":\"Line\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"2523dcc8-e9a5-456b-a89f-26703d26f2c4\",\"type\":\"PanTool\"},{\"id\":\"e228a179-a029-44ca-8710-76f36e224a50\",\"type\":\"WheelZoomTool\"},{\"id\":\"87b48be0-222b-4995-9cf1-852fb35f5224\",\"type\":\"BoxZoomTool\"},{\"id\":\"cc5ee919-318f-47d6-8713-649a441949ca\",\"type\":\"SaveTool\"},{\"id\":\"10d10277-25c8-4498-a8ac-5eecc7482113\",\"type\":\"ResetTool\"},{\"id\":\"ab29a5dd-bf1f-4812-a1f4-96bfdeab86fb\",\"type\":\"HelpTool\"}]},\"id\":\"25bf169e-c15a-4ffa-978e-9adff52abd89\",\"type\":\"Toolbar\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b92e4664-9909-4baa-9fc1-23d1d833344a\",\"type\":\"BasicTicker\"}},\"id\":\"b85ff5d9-6e7d-4317-be74-30083b335618\",\"type\":\"Grid\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"987a3d0e-177e-4ba6-8703-932d940a1adf\",\"type\":\"Line\"},{\"attributes\":{\"label\":{\"value\":\"Avg Cost\"},\"renderers\":[{\"id\":\"aee2f226-8289-4112-bdd4-dc679e001c53\",\"type\":\"GlyphRenderer\"}]},\"id\":\"2d0aa1df-0f1b-4cf2-a156-b0bad581cf10\",\"type\":\"LegendItem\"},{\"attributes\":{\"sizing_mode\":\"scale_width\",\"toolbar_location\":\"above\",\"tools\":[{\"id\":\"a8c7ccc0-a655-467c-8f9f-b9fa3dae2700\",\"type\":\"PanTool\"},{\"id\":\"269feea5-7165-40e8-b3d9-3cfc5ad8627a\",\"type\":\"WheelZoomTool\"},{\"id\":\"7e646d43-718a-403b-b6e2-249d40c1c03f\",\"type\":\"BoxZoomTool\"},{\"id\":\"5f4d6931-cb2d-406e-8506-97e2abbf7643\",\"type\":\"SaveTool\"},{\"id\":\"e1c096d5-8824-4ee9-98c0-65ebc1112062\",\"type\":\"ResetTool\"},{\"id\":\"ea72b81e-b2c8-469f-ae87-7f05054e5bbd\",\"type\":\"HelpTool\"},{\"id\":\"2523dcc8-e9a5-456b-a89f-26703d26f2c4\",\"type\":\"PanTool\"},{\"id\":\"e228a179-a029-44ca-8710-76f36e224a50\",\"type\":\"WheelZoomTool\"},{\"id\":\"87b48be0-222b-4995-9cf1-852fb35f5224\",\"type\":\"BoxZoomTool\"},{\"id\":\"cc5ee919-318f-47d6-8713-649a441949ca\",\"type\":\"SaveTool\"},{\"id\":\"10d10277-25c8-4498-a8ac-5eecc7482113\",\"type\":\"ResetTool\"},{\"id\":\"ab29a5dd-bf1f-4812-a1f4-96bfdeab86fb\",\"type\":\"HelpTool\"}]},\"id\":\"92629c4a-4e5d-4c73-8b22-29b2859e9be6\",\"type\":\"ToolbarBox\"},{\"attributes\":{},\"id\":\"b92e4664-9909-4baa-9fc1-23d1d833344a\",\"type\":\"BasicTicker\"},{\"attributes\":{\"line_color\":{\"value\":\"navy\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1866d002-afbe-4382-a6d8-9a5f4b869e32\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"57eb1d1b-a2aa-4e15-b8a2-d2ad61690ec2\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"children\":[{\"id\":\"80f08ba2-1cba-4835-a6c9-0ae77bf55848\",\"type\":\"Row\"},{\"id\":\"fbe1e72b-ff64-4c8a-821c-3050460d1a08\",\"type\":\"Row\"}]},\"id\":\"dab1febb-5625-440c-ade2-5cf1e610c4a4\",\"type\":\"Column\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"a8c7ccc0-a655-467c-8f9f-b9fa3dae2700\",\"type\":\"PanTool\"},{\"id\":\"269feea5-7165-40e8-b3d9-3cfc5ad8627a\",\"type\":\"WheelZoomTool\"},{\"id\":\"7e646d43-718a-403b-b6e2-249d40c1c03f\",\"type\":\"BoxZoomTool\"},{\"id\":\"5f4d6931-cb2d-406e-8506-97e2abbf7643\",\"type\":\"SaveTool\"},{\"id\":\"e1c096d5-8824-4ee9-98c0-65ebc1112062\",\"type\":\"ResetTool\"},{\"id\":\"ea72b81e-b2c8-469f-ae87-7f05054e5bbd\",\"type\":\"HelpTool\"}]},\"id\":\"1afe2f98-e894-484d-9d8d-5a9d1bf16285\",\"type\":\"Toolbar\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"f44030d2-ee52-4116-b1ea-ed9dd3041386\",\"type\":\"Line\"},{\"attributes\":{\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"e228a179-a029-44ca-8710-76f36e224a50\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"below\":[{\"id\":\"bd53f2e7-8bcf-45d2-ad2f-19df3e819f43\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"ae625fe8-c649-4518-bd54-450c90a4e648\",\"type\":\"LinearAxis\"}],\"plot_height\":300,\"renderers\":[{\"id\":\"bd53f2e7-8bcf-45d2-ad2f-19df3e819f43\",\"type\":\"LinearAxis\"},{\"id\":\"1f5ef60e-e008-4a55-bc1e-3b5bfac0caea\",\"type\":\"Grid\"},{\"id\":\"ae625fe8-c649-4518-bd54-450c90a4e648\",\"type\":\"LinearAxis\"},{\"id\":\"b85ff5d9-6e7d-4317-be74-30083b335618\",\"type\":\"Grid\"},{\"id\":\"24d9d55a-075e-4bab-ae05-96e1dc462461\",\"type\":\"BoxAnnotation\"},{\"id\":\"42e96c63-20a1-4a74-a8ad-2bf488316f22\",\"type\":\"Legend\"},{\"id\":\"aee2f226-8289-4112-bdd4-dc679e001c53\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"132d3401-d4a1-48e3-9cf4-4c9c26c958f4\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"7d019933-0e1e-4feb-85f1-4eaf80fb01ec\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"1afe2f98-e894-484d-9d8d-5a9d1bf16285\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"835b2c52-f1c9-4a26-aa17-1b355c79f93c\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"d4640cd9-f802-4bdf-8e7c-32f7170f3252\",\"type\":\"DataRange1d\"}},\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"4fb71642-4700-453e-a274-e65dd33733ce\",\"type\":\"BasicTicker\"}},\"id\":\"4778d811-0e2f-4ff1-8bb4-169cac765236\",\"type\":\"Grid\"},{\"attributes\":{\"label\":{\"value\":\"Training\"},\"renderers\":[{\"id\":\"9785489e-3e7b-4d5f-9297-09cbec013dc8\",\"type\":\"GlyphRenderer\"}]},\"id\":\"b20be46d-b9ad-4100-ac1b-d0efec37c569\",\"type\":\"LegendItem\"},{\"attributes\":{\"callback\":null},\"id\":\"f5634098-29bf-4035-83da-f98c358d6780\",\"type\":\"DataRange1d\"},{\"attributes\":{\"formatter\":{\"id\":\"3e936af5-1422-41a0-9457-ebd009dea7ee\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"4fb71642-4700-453e-a274-e65dd33733ce\",\"type\":\"BasicTicker\"}},\"id\":\"0064df07-3c12-45d3-a54c-7e9bd633cce1\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"3e936af5-1422-41a0-9457-ebd009dea7ee\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_color\":{\"value\":\"firebrick\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"7e9812f3-3982-4982-b913-e8d16bb0710d\",\"type\":\"Line\"},{\"attributes\":{\"overlay\":{\"id\":\"5a0d3e74-a7b0-4966-8ef1-a8a44ade9a42\",\"type\":\"BoxAnnotation\"},\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"87b48be0-222b-4995-9cf1-852fb35f5224\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"d4640cd9-f802-4bdf-8e7c-32f7170f3252\",\"type\":\"DataRange1d\"},{\"attributes\":{\"formatter\":{\"id\":\"501bee41-29c9-420d-b384-6b210c5022c2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c35f63ce-58a6-4325-99cf-8d6cbe6e18f8\",\"type\":\"BasicTicker\"}},\"id\":\"bd53f2e7-8bcf-45d2-ad2f-19df3e819f43\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"10d10277-25c8-4498-a8ac-5eecc7482113\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"dc862eb5-ce71-495a-8b37-05ced3674b75\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"24d9d55a-075e-4bab-ae05-96e1dc462461\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"c35f63ce-58a6-4325-99cf-8d6cbe6e18f8\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"5f4d6931-cb2d-406e-8506-97e2abbf7643\",\"type\":\"SaveTool\"},{\"attributes\":{\"children\":[{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"fbe1e72b-ff64-4c8a-821c-3050460d1a08\",\"type\":\"Row\"},{\"attributes\":{\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2e8379f6-25ec-4d35-99e5-156466f75922\",\"type\":\"BasicTicker\"}},\"id\":\"f5e0752d-2953-4dc6-ac08-6a23e858bc89\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"a8d51af1-c6a7-4fff-a20c-50008935e103\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"formatter\":{\"id\":\"dc862eb5-ce71-495a-8b37-05ced3674b75\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b92e4664-9909-4baa-9fc1-23d1d833344a\",\"type\":\"BasicTicker\"}},\"id\":\"ae625fe8-c649-4518-bd54-450c90a4e648\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"6991bc4e-7fdd-44d5-8835-62aa4a0760bb\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":null,\"text\":\"Average Training Cost Graph\"},\"id\":\"132d3401-d4a1-48e3-9cf4-4c9c26c958f4\",\"type\":\"Title\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"5a0d3e74-a7b0-4966-8ef1-a8a44ade9a42\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"57eb1d1b-a2aa-4e15-b8a2-d2ad61690ec2\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7e9812f3-3982-4982-b913-e8d16bb0710d\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"f44030d2-ee52-4116-b1ea-ed9dd3041386\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"952dc3b9-86cc-4cea-8687-d3bb63f85ef9\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":null,\"text\":\"Training/Validation Accuracy Graph\"},\"id\":\"46859bdc-e27e-4a14-aa8a-8a917b4d69bf\",\"type\":\"Title\"},{\"attributes\":{\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"cc5ee919-318f-47d6-8713-649a441949ca\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":[],\"y\":[]}},\"id\":\"160e456b-658d-4089-9434-79a39a6717c7\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"160e456b-658d-4089-9434-79a39a6717c7\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"e26f4acc-ca40-46cd-84cf-55817b29e790\",\"type\":\"Line\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"835a2895-4e3e-44b8-92f0-8b8a6443669a\",\"type\":\"Line\"},\"selection_glyph\":null},\"id\":\"aee2f226-8289-4112-bdd4-dc679e001c53\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"dcd7a809-9204-447a-9c0b-511b321d60c0\",\"type\":\"ToolEvents\"},{\"attributes\":{\"items\":[{\"id\":\"b20be46d-b9ad-4100-ac1b-d0efec37c569\",\"type\":\"LegendItem\"},{\"id\":\"a097d194-2835-4bbc-8c75-9c724f9c0ac1\",\"type\":\"LegendItem\"}],\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b19eab68-fa5e-4f25-83ab-d442a3ecab18\",\"type\":\"Legend\"},{\"attributes\":{\"label\":{\"value\":\"Validation\"},\"renderers\":[{\"id\":\"952dc3b9-86cc-4cea-8687-d3bb63f85ef9\",\"type\":\"GlyphRenderer\"}]},\"id\":\"a097d194-2835-4bbc-8c75-9c724f9c0ac1\",\"type\":\"LegendItem\"},{\"attributes\":{\"plot\":{\"id\":\"883f7eb8-e478-4a4d-b801-6c2dd98178d0\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"ab29a5dd-bf1f-4812-a1f4-96bfdeab86fb\",\"type\":\"HelpTool\"},{\"attributes\":{\"plot\":{\"id\":\"8a09896d-963f-454c-8c10-388b35a97fa2\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"a8c7ccc0-a655-467c-8f9f-b9fa3dae2700\",\"type\":\"PanTool\"}],\"root_ids\":[\"0a43d5e9-2e0b-46cb-8ff2-6754ef2c97ae\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.3\"}};\n",
       "            var render_items = [{\"docid\":\"d6286289-5738-44d2-8e96-76b5f816ee1d\",\"elementid\":\"8106c150-6080-427d-90d7-8a22ef54c27a\",\"modelid\":\"0a43d5e9-2e0b-46cb-8ff2-6754ef2c97ae\",\"notebook_comms_target\":\"313e8ccd-0dff-4252-bc4f-bae5d0e21d76\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        });\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === \"1\") {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (!force) {\n",
       "        var cell = $(\"#8106c150-6080-427d-90d7-8a22ef54c27a\").parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><code>&lt;Bokeh Notebook handle for <strong>In[5]</strong>&gt;</code></p>"
      ],
      "text/plain": [
       "<bokeh.io._CommsHandle at 0x7fd3bff10160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.io import push_notebook, show, output_notebook, save\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()\n",
    "\n",
    "epoch_values=[]\n",
    "cost_values=[]\n",
    "training_accuracy_values=[]\n",
    "validation_accuracy_values=[]\n",
    "\n",
    "p_cost = figure(title=\"Average Training Cost Graph\", plot_height=300, plot_width=600)\n",
    "r_cost = p_cost.line(epoch_values, cost_values, color=\"navy\", line_width=1, legend=\"Avg Cost\")\n",
    "\n",
    "p_accuracy = figure(title=\"Training/Validation Accuracy Graph\", plot_height=300, plot_width=600)\n",
    "r_train_accuracy = p_accuracy.line(epoch_values, training_accuracy_values, color=\"navy\", line_width=1, legend=\"Training\")\n",
    "r_test_accuracy = p_accuracy.line(epoch_values, validation_accuracy_values, color=\"firebrick\", line_width=1, legend=\"Validation\")\n",
    "\n",
    "grid = gridplot([[p_cost],[p_accuracy]])\n",
    "\n",
    "def update(newx, newcost, newtrain, newtest):\n",
    "    r_cost.data_source.data['x'] = newx\n",
    "    r_cost.data_source.data['y'] = newcost\n",
    "    \n",
    "    r_train_accuracy.data_source.data['x'] = newx\n",
    "    r_train_accuracy.data_source.data['y'] = newtrain\n",
    "    \n",
    "    r_test_accuracy.data_source.data['x'] = newx\n",
    "    r_test_accuracy.data_source.data['y'] = newtest\n",
    "    push_notebook()\n",
    "\n",
    "show(grid, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-8c670c362d17>:52: SyntaxWarning: name 'current_row' is assigned to before global declaration\n",
      "  global current_row\n",
      "<ipython-input-6-8c670c362d17>:56: SyntaxWarning: name 'current_row' is assigned to before global declaration\n",
      "  global current_row\n"
     ]
    }
   ],
   "source": [
    "def run_training_testing(modelname, filename, vectortype, train_validation_test, layers):\n",
    "    if vectortype == 'word2vec':\n",
    "        n_input = 1000\n",
    "    else:\n",
    "        n_input = 10\n",
    "    \n",
    "    max_epochs = 1000\n",
    "    display_epoch_step = 1\n",
    "    batch_size = 100\n",
    "    \n",
    "    x, y, cost, optimizer, predict_op = create_architecture(n_input, layers)\n",
    "    \n",
    "    train_features, train_labels, validation_features, validation_labels,  test_features, test_labels = create_features_labels(filename, vectortype, train_validation_test)\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Launch the graph\n",
    "    current_row = 0 \n",
    "    num_examples = len(train_labels)\n",
    "    print('number of samples - {}'.format(num_examples))\n",
    "\n",
    "    prev_validation_accuracy = 0\n",
    "    MAX_FAIL = 3\n",
    "    \n",
    "    classify_func = np.vectorize(lambda x: 1 if x >=0.5 else 0)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training cycle\n",
    "        print('running for maximum of {epochs} epochs or validation failed {MAX_FAIL} times'.format(\n",
    "                epochs=max_epochs, \n",
    "                MAX_FAIL=MAX_FAIL))\n",
    "\n",
    "        epoch = 0\n",
    "        prev_cost = None\n",
    "        epsilon = 999\n",
    "        fail_count = 0\n",
    "        while True:\n",
    "            \n",
    "            if epoch >= max_epochs:\n",
    "                break\n",
    "            epoch += 1\n",
    "\n",
    "            epoch_cost = 0.\n",
    "            total_batch = int(num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            global current_row\n",
    "            current_row = 0\n",
    "            for i in range(total_batch):\n",
    "\n",
    "                global current_row\n",
    "                start = current_row\n",
    "                current_row += batch_size\n",
    "                end = current_row\n",
    "\n",
    "                \"\"\"\n",
    "                print(start,end)\n",
    "                if i >= 10:\n",
    "                    break\"\"\"\n",
    "\n",
    "                batch_x = train_features[start:end]\n",
    "                batch_y = train_labels[start:end]\n",
    "\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "                c = sess.run(cost, feed_dict = {x: batch_x, y: batch_y})\n",
    "                #_, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                # Compute average loss\n",
    "                epoch_cost += c\n",
    "            # Display logs per epoch step\n",
    "            current_cost = epoch_cost/total_batch\n",
    "            epsilon = ((prev_cost - current_cost) if prev_cost else current_cost)\n",
    "            prev_cost = current_cost\n",
    "            \n",
    "            training_prediction = sess.run(predict_op, feed_dict = {x: train_features, y: train_labels})\n",
    "            training_accuracy = accuracy_score(train_labels, classify_func(training_prediction))\n",
    "            validation_prediction = sess.run(predict_op, feed_dict = {x: validation_features, y: validation_labels})\n",
    "            validation_accuracy = accuracy_score(validation_labels, classify_func(validation_prediction))\n",
    "            \n",
    "            if epoch % display_epoch_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch), \n",
    "                      \"cost=\", \"{:.9f}\".format(current_cost),\n",
    "                      \"training accuracy= {}\".format(training_accuracy),\n",
    "                      \"validation accuracy= {}\".format(validation_accuracy)\n",
    "                     )\n",
    "            \n",
    "            # Add values to graph lists\n",
    "            epoch_values.append(epoch)\n",
    "            cost_values.append(current_cost)\n",
    "            training_accuracy_values.append(training_accuracy)\n",
    "            validation_accuracy_values.append(validation_accuracy)\n",
    "            update(epoch_values,cost_values,training_accuracy_values,validation_accuracy_values)\n",
    "            \n",
    "            \n",
    "            if validation_accuracy > prev_validation_accuracy:\n",
    "                print(\"saving new best validation model - {}\".format(validation_accuracy))\n",
    "                saver.save(sess, \"bestmodel.ckpt\")\n",
    "                fail_count = 0\n",
    "            else:\n",
    "                print(\"reduced validation acc!\")\n",
    "                fail_count += 1\n",
    "                if fail_count >= MAX_FAIL:\n",
    "                    break\n",
    "                    \n",
    "            prev_validation_accuracy =  validation_accuracy\n",
    "                    \n",
    "        print(\"Optimization Finished!\")\n",
    "        \n",
    "        saver.restore(sess, \"bestmodel.ckpt\")\n",
    "        \n",
    "        predicts, cost_ = sess.run([predict_op, cost], feed_dict = {x: test_features, y: test_labels})\n",
    "        print(predicts)\n",
    "        final_accuracy = accuracy_score(test_labels, classify_func(predicts))\n",
    "        final_auc = roc_auc_score(test_labels, predicts)\n",
    "        final_prfs = precision_recall_fscore_support(test_labels, classify_func(predicts), average ='binary')\n",
    "        final_cost = cost_ / len(test_labels)\n",
    "        \n",
    "        print('final results:',\n",
    "              'accuracy:', final_accuracy,\n",
    "              'auc:', final_auc,\n",
    "              'cost:', final_cost,\n",
    "              'precision,recall,fscore,support',final_prfs,\n",
    "              'epoch:', epoch\n",
    "             )\n",
    "        \n",
    "        with open('test_results.txt','a') as wfile:\n",
    "            wfile.write('modelname - {modelname}, accuracy: {accuracy}, auc: {auc}, prfs: {prfs}, cost: {cost}, epoch: {epoch}'.format(\n",
    "                    modelname = modelname,\n",
    "                    accuracy = final_accuracy,\n",
    "                    auc = final_auc,\n",
    "                    cost = final_cost,\n",
    "                    prfs = final_prfs,\n",
    "                    epoch = epoch\n",
    "                ) + '\\n')\n",
    "        # Test model\n",
    "        #correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        #print(\"Accuracy:\", accuracy.eval({x: train_features, y: train_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180023, 2)\n",
      "BUSINESS\n",
      "0    129253\n",
      "1     50770\n",
      "Name: BUSINESS, dtype: int64\n",
      "Train-\n",
      "BUSINESS\n",
      "0    90477\n",
      "1    35539\n",
      "Name: BUSINESS, dtype: int64\n",
      "Validation-\n",
      "BUSINESS\n",
      "0    12926\n",
      "1     5077\n",
      "Name: BUSINESS, dtype: int64\n",
      "Test-\n",
      "BUSINESS\n",
      "0    25850\n",
      "1    10154\n",
      "Name: BUSINESS, dtype: int64\n",
      "number of samples - 126016\n",
      "running for maximum of 1000 epochs or validation failed 3 times\n",
      "Epoch: 0001 cost= 26663110658.031745911 training accuracy= 0.7401917216861351 validation accuracy= 0.7412653446647781\n",
      "saving new best validation model - 0.7412653446647781\n",
      "Epoch: 0002 cost= 13329209258.869840622 training accuracy= 0.77554437531742 validation accuracy= 0.7730378270288285\n",
      "saving new best validation model - 0.7730378270288285\n",
      "Epoch: 0003 cost= 9335097841.168254852 training accuracy= 0.7929310563737938 validation accuracy= 0.7891462534022108\n",
      "saving new best validation model - 0.7891462534022108\n",
      "Epoch: 0004 cost= 7018162674.692063332 training accuracy= 0.8023901726764855 validation accuracy= 0.7963672721213131\n",
      "saving new best validation model - 0.7963672721213131\n",
      "Epoch: 0005 cost= 5444750581.536507607 training accuracy= 0.8122778059928898 validation accuracy= 0.8034216519468977\n",
      "saving new best validation model - 0.8034216519468977\n",
      "Epoch: 0006 cost= 4295775371.784127235 training accuracy= 0.8198879507364144 validation accuracy= 0.8086430039437872\n",
      "saving new best validation model - 0.8086430039437872\n",
      "Epoch: 0007 cost= 3433156137.142857075 training accuracy= 0.8280059674961909 validation accuracy= 0.8139754485363551\n",
      "saving new best validation model - 0.8139754485363551\n",
      "Epoch: 0008 cost= 2767318917.841269970 training accuracy= 0.8351082402234636 validation accuracy= 0.8188079764483697\n",
      "saving new best validation model - 0.8188079764483697\n",
      "Epoch: 0009 cost= 2245040594.361904621 training accuracy= 0.8413455434230573 validation accuracy= 0.8226406709992778\n",
      "saving new best validation model - 0.8226406709992778\n",
      "Epoch: 0010 cost= 1833770491.073015928 training accuracy= 0.8466543930929405 validation accuracy= 0.8258623562739543\n",
      "saving new best validation model - 0.8258623562739543\n",
      "Epoch: 0011 cost= 1505673155.657142878 training accuracy= 0.8514633062468258 validation accuracy= 0.8278064766983281\n",
      "saving new best validation model - 0.8278064766983281\n",
      "Epoch: 0012 cost= 1242969019.657142878 training accuracy= 0.8565102844083291 validation accuracy= 0.8297505971227018\n",
      "saving new best validation model - 0.8297505971227018\n",
      "Epoch: 0013 cost= 1032150397.358730197 training accuracy= 0.8610493905535805 validation accuracy= 0.8331944675887352\n",
      "saving new best validation model - 0.8331944675887352\n",
      "Epoch: 0014 cost= 860669417.326984167 training accuracy= 0.8651282376841036 validation accuracy= 0.8362495139698939\n",
      "saving new best validation model - 0.8362495139698939\n",
      "Epoch: 0015 cost= 720135538.107936502 training accuracy= 0.8692467623158964 validation accuracy= 0.8382491806921069\n",
      "saving new best validation model - 0.8382491806921069\n",
      "Epoch: 0016 cost= 605398220.209523797 training accuracy= 0.8739683849669884 validation accuracy= 0.8395267455424096\n",
      "saving new best validation model - 0.8395267455424096\n",
      "Epoch: 0017 cost= 511724984.860317469 training accuracy= 0.8779440705942103 validation accuracy= 0.8405821252013553\n",
      "saving new best validation model - 0.8405821252013553\n",
      "Epoch: 0018 cost= 434216521.533333361 training accuracy= 0.8806500761808025 validation accuracy= 0.8444148197522635\n",
      "saving new best validation model - 0.8444148197522635\n",
      "Epoch: 0019 cost= 369970845.819047630 training accuracy= 0.8827212417470798 validation accuracy= 0.8446925512414598\n",
      "saving new best validation model - 0.8446925512414598\n",
      "Epoch: 0020 cost= 317938415.003174603 training accuracy= 0.8858319578466226 validation accuracy= 0.8474143198355829\n",
      "saving new best validation model - 0.8474143198355829\n",
      "Epoch: 0021 cost= 272956172.671428561 training accuracy= 0.8911487430167597 validation accuracy= 0.8466366716658335\n",
      "reduced validation acc!\n",
      "Epoch: 0022 cost= 236435195.455555558 training accuracy= 0.8934341670898933 validation accuracy= 0.8488029772815642\n",
      "saving new best validation model - 0.8488029772815642\n",
      "Epoch: 0023 cost= 204387774.934920639 training accuracy= 0.897568562722194 validation accuracy= 0.8493028939621174\n",
      "saving new best validation model - 0.8493028939621174\n",
      "Epoch: 0024 cost= 176848662.622619033 training accuracy= 0.9002110843067547 validation accuracy= 0.8500805421318669\n",
      "saving new best validation model - 0.8500805421318669\n",
      "Epoch: 0025 cost= 154498804.564682543 training accuracy= 0.8989572752666328 validation accuracy= 0.8538576903849359\n",
      "saving new best validation model - 0.8538576903849359\n",
      "Epoch: 0026 cost= 135276922.215079367 training accuracy= 0.902282249873032 validation accuracy= 0.8534688663000611\n",
      "reduced validation acc!\n",
      "Epoch: 0027 cost= 119070038.280158728 training accuracy= 0.9013934738445911 validation accuracy= 0.857023829361773\n",
      "saving new best validation model - 0.857023829361773\n",
      "Epoch: 0028 cost= 105056948.812698409 training accuracy= 0.9001793423057389 validation accuracy= 0.8556907182136311\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 94564023.143353179 training accuracy= 0.9116699466734383 validation accuracy= 0.8558573571071488\n",
      "saving new best validation model - 0.8558573571071488\n",
      "Epoch: 0030 cost= 83866689.293055549 training accuracy= 0.915185373285932 validation accuracy= 0.8573015608509693\n",
      "saving new best validation model - 0.8573015608509693\n",
      "Epoch: 0031 cost= 76320747.948983133 training accuracy= 0.907852971051295 validation accuracy= 0.8594123201688607\n",
      "saving new best validation model - 0.8594123201688607\n",
      "Epoch: 0032 cost= 68869430.831547618 training accuracy= 0.9201609319451498 validation accuracy= 0.858801310892629\n",
      "reduced validation acc!\n",
      "Epoch: 0033 cost= 66210108.866071425 training accuracy= 0.921256030980193 validation accuracy= 0.8618008109759484\n",
      "saving new best validation model - 0.8618008109759484\n",
      "Epoch: 0034 cost= 60022105.269791670 training accuracy= 0.9234065515490096 validation accuracy= 0.8631894684219297\n",
      "saving new best validation model - 0.8631894684219297\n",
      "Epoch: 0035 cost= 56611052.215773806 training accuracy= 0.9269695911630269 validation accuracy= 0.8628561906348942\n",
      "reduced validation acc!\n",
      "Epoch: 0036 cost= 52928823.577232145 training accuracy= 0.9270965591670899 validation accuracy= 0.8642448480808754\n",
      "saving new best validation model - 0.8642448480808754\n",
      "Epoch: 0037 cost= 50960844.893551588 training accuracy= 0.9284059167089893 validation accuracy= 0.8656335055268566\n",
      "saving new best validation model - 0.8656335055268566\n",
      "Epoch: 0038 cost= 50195528.980580360 training accuracy= 0.9320800533265617 validation accuracy= 0.8639671165916791\n",
      "reduced validation acc!\n",
      "Epoch: 0039 cost= 46668610.286557540 training accuracy= 0.9118683341797867 validation accuracy= 0.8515802921735266\n",
      "reduced validation acc!\n",
      "Epoch: 0040 cost= 44375937.412940226 training accuracy= 0.8750714195022854 validation accuracy= 0.8315280786535577\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "final results: accuracy: 0.868959004555 auc: 0.825802037405 cost: 1506728.2302 precision,recall,fscore,support (0.79150579150579148, 0.72680716958833957, 0.75777800595543687, None) epoch: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/bokeh/io.py:433: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warnings.warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/bokeh/io.py:443: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warnings.warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/chart_space_word2vec_model_05_layers3.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validation_test = split_test_validation('space_ngram_30krows_word2vec_05_threshold_new.pickle')\n",
    "\n",
    "epoch_values=[]\n",
    "cost_values=[]\n",
    "training_accuracy_values=[]\n",
    "validation_accuracy_values=[]\n",
    "update(epoch_values,cost_values,training_accuracy_values,validation_accuracy_values)\n",
    "model_name = 'space_word2vec_model_05_layers3'\n",
    "\n",
    "run_training_testing(model_name, 'space_ngram_30krows_word2vec_05_threshold_new.pickle', 'word2vec', train_validation_test, 3)\n",
    "save(grid, 'chart_{}.html'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename= 'space_ngram_30krows_word2vec_10_threshold.pickle'\n",
    "x, y, cost, optimizer, predict_op = create_architecture(1000, 3)\n",
    "train_validation_test = split_test_validation(filename)\n",
    "train_features, train_labels, validation_features, validation_labels,  test_features, test_labels = create_features_labels(filename, 'word2vec', train_validation_test)\n",
    "saver = tf.train.Saver()\n",
    "classify_func = np.vectorize(lambda x: 1 if x >=0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "final results: accuracy: 0.916266575217 auc: 0.916266575217 cost: 10831214.6173\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"bestmodel.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    predicts, cost_ = sess.run([predict_op, cost], feed_dict = {x: test_features, y: test_labels})\n",
    "    print(predicts)\n",
    "    final_accuracy = accuracy_score(test_labels, classify_func(predicts))\n",
    "    final_auc = roc_auc_score(test_labels, predicts)\n",
    "    final_cost = cost_ / len(test_labels)\n",
    "\n",
    "    print('final results:',\n",
    "          'accuracy:', final_accuracy,\n",
    "          'auc:', final_auc,\n",
    "          'cost:', final_cost\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "nltk.download(info_or_id='stopwords')\n",
    "nltk.download(info_or_id='punkt')\n",
    "sw = set(stopwords.words(\"english\")) | set(i.strip() for i in list(open('stopwordlist.txt'))[1:])\n",
    "terms_df = pd.read_csv('finalterms_grouped_clean2.csv')\n",
    "single_terms = set(b for a in terms_df['term'] for b in nltk.word_tokenize(a) if (b not in sw and len(b)> 1))\n",
    "del stopwords\n",
    "del terms_df\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "puncs = set(string.punctuation)\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "def return_words_from_text(x):\n",
    "    def return_tokens(fulltext,start,end):\n",
    "        text = fulltext[start:end]\n",
    "        return tuple( [\n",
    "            tuple(t.strip(string.punctuation) for t in tokenizer.tokenize(re.sub(r'\\d+','SOMENUM',text.lower()))), \n",
    "            tuple((t[0]+start,t[1]+start) for t in tokenizer.span_tokenize(text))\n",
    "                ]  )\n",
    "    return tuple(return_tokens(x.strip(), sent[0], sent[1]) for sent in sent_detector.span_tokenize(x.strip()))\n",
    "\n",
    "def get_ngrams(x, ngram_start, ngram_end):\n",
    "    def get_ngram(sent, i):\n",
    "        span = [(s[0][0],s[-1][-1]) for s in ngrams(sent[1], i)]\n",
    "        the_ngrams = list(ngrams(sent[0], i))\n",
    "        #print(len(span))\n",
    "        #print(len(the_ngrams))\n",
    "        if len(the_ngrams) > 0:\n",
    "            return zip(the_ngrams, span)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def remove_punc(ngram):\n",
    "        return tuple([tuple(gram for gram in ngram[0] if not all(j in puncs for j in gram)), ngram[1]])\n",
    "    def check_num_only(ngram):\n",
    "        #print(ngram)\n",
    "        return set(ngram[0]) != set('SOMENUM')\n",
    "          \n",
    "    ngram_list = defaultdict(list)\n",
    "    for sent in x:\n",
    "        #print(sent)\n",
    "        for i in range(ngram_start,ngram_end+1):\n",
    "            for gram in map(remove_punc, get_ngram(sent, i)):\n",
    "                if ~check_num_only(gram):\n",
    "                    ngram_list[gram[0]].append(gram[1])\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:Pattern library is not installed, lemmatization won't be available.\n",
      "INFO:gensim.corpora.sharded_corpus:Could not import Theano, will use standard float for default ShardedCorpus dtype.\n",
      "INFO:summa.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n",
      "INFO:gensim.utils:loading Word2Vec object from space_100features_5minwords_10context_300karticles.bin\n",
      "INFO:gensim.utils:loading syn1neg from space_100features_5minwords_10context_300karticles.bin.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn0 from space_100features_5minwords_10context_300karticles.bin.syn0.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:setting ignored attribute syn0norm to None\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"space_100features_5minwords_10context_300karticles.bin\")\n",
    "allwords = set(model.index2word)\n",
    "def get_vector(term):\n",
    "    vector = [model[unigram] if unigram in allwords else np.zeros(100) for unigram in term.lower().split(' ') ]\n",
    "    return np.concatenate(vector + [np.zeros(100)]*(10-len(vector)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = \"\"\"AT&T will pay $107.50 for each Time Warner share, in a combination of cash and stock, worth $85.4bn overall, according to a statement.\n",
    "AT&T said it expected to close the deal to be completed by the end of 2017.\n",
    "Other media company shares, including Discovery, AMC, Netflix and CBS, recently rose as investors speculated that a deal could spark a fresh wave of takeovers and mergers among media and technology companies.\n",
    "AT&T, which has a market value of about $238bn, has already made moves to turn itself into a media powerhouse, buying satellite TV provider DirecTV last year for $48.5bn.\n",
    "Time Warner chief executive Jeff Bewkes has, however, resisted selling in the past. The company rejected an $80bn offer from Twenty-First Century Fox Inc in 2014.\n",
    "The deal gives AT&T access to a major producer of content as it seeks to diversify away from its core telecoms business. Rival Verizon is currently in negotiations to buy Yahoo and has already bought AOL, owner of Huffington Post.\n",
    "Some analysts, however, question whether AT&T needs to mount a complete takeover of Time Warner.\n",
    "\"\"\"\n",
    "#new_text = new_text.replace(\"\\n\",\" \")\n",
    "ngram_of_article = get_ngrams(return_words_from_text(new_text), 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x, y, cost, optimizer, predict_op = create_architecture(1000, 3)\n",
    "saver = tf.train.Saver()\n",
    "classify_func = np.vectorize(lambda x: 1 if x >=0.5 else 0)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"bestmodel_10_new3.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    for i in ngram_of_article:\n",
    "        vector = np.array([0.000001 if y==0 else y for y in get_vector(i[0]) ])\n",
    "        #print(ngram_of_article[i][1])\n",
    "        predicts = sess.run(predict_op, feed_dict = {x: [vector]})\n",
    "        ngram_of_article[i] = (ngram_of_article[i], vector, classify_func(predicts)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('turn', 'itself', 'into')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each', 'time')\n",
      "('value', 'of', 'about', 'SOMENUMbn')\n",
      "('deal', 'could', 'spark', 'a', 'fresh', 'wave', 'of', 'takeovers')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each', 'time', 'warner')\n",
      "('share', 'in', 'a', 'combination', 'of', 'cash', 'and', 'stock', 'worth', 'SOMENUM.SOMENUMbn')\n",
      "('investors', 'speculated', 'that', 'a')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each')\n",
      "('negotiations', 'to', 'buy')\n",
      "('end', 'of', 'SOMENUM')\n",
      "('market',)\n",
      "('buy', 'yahoo', 'and', 'has', 'already', 'bought')\n",
      "('offer', 'from', 'twenty-first', 'century', 'fox', 'inc')\n",
      "('SOMENUMbn', 'has', 'already', 'made', 'moves', 'to', 'turn', 'itself', 'into')\n",
      "('value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made', 'moves', 'to')\n",
      "('expected', 'to', 'close')\n",
      "('discovery', 'amc', 'netflix', 'and', 'cbs', 'recently', 'rose', 'as', 'investors', 'speculated')\n",
      "('past',)\n",
      "('takeover', 'of')\n",
      "('moves', 'to', 'turn', 'itself', 'into')\n",
      "('time', 'warner', 'share', 'in', 'a', 'combination', 'of', 'cash', 'and', 'stock')\n",
      "('takeover', 'of', 'time')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer', 'from')\n",
      "('twenty-first', 'century')\n",
      "('powerhouse', 'buying', 'satellite', 'tv', 'provider', 'directv', 'last', 'year', 'for', 'SOMENUM.SOMENUMbn')\n",
      "('combination', 'of', 'cash', 'and', 'stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to')\n",
      "('discovery', 'amc', 'netflix')\n",
      "('moves', 'to', 'turn', 'itself', 'into', 'a', 'media')\n",
      "('stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall')\n",
      "('investors', 'speculated', 'that', 'a', 'deal', 'could', 'spark', 'a', 'fresh')\n",
      "('provider', 'directv', 'last', 'year', 'for')\n",
      "('turn', 'itself', 'into', 'a', 'media')\n",
      "('expected', 'to', 'close', 'the', 'deal', 'to', 'be', 'completed')\n",
      "('deal', 'to', 'be', 'completed', 'by', 'the', 'end', 'of')\n",
      "('investors', 'speculated')\n",
      "('value', 'of', 'about')\n",
      "('deal', 'to', 'be', 'completed')\n",
      "('deal', 'gives', 'at&t', 'access', 'to', 'a', 'major', 'producer')\n",
      "('bought', 'aol', 'owner', 'of', 'huffington', 'post')\n",
      "('turn', 'itself', 'into', 'a', 'media', 'powerhouse', 'buying', 'satellite', 'tv')\n",
      "('rejected', 'an', 'SOMENUMbn')\n",
      "('completed', 'by', 'the', 'end', 'of', 'SOMENUM')\n",
      "('completed', 'by', 'the', 'end')\n",
      "('combination', 'of', 'cash')\n",
      "('combination', 'of', 'cash', 'and', 'stock', 'worth', 'SOMENUM.SOMENUMbn')\n",
      "('expected',)\n",
      "('time', 'warner', 'chief', 'executive', 'jeff', 'bewkes')\n",
      "('investors', 'speculated', 'that')\n",
      "('twenty-first', 'century', 'fox', 'inc')\n",
      "('share', 'in', 'a')\n",
      "('offer', 'from', 'twenty-first')\n",
      "('stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall', 'according')\n",
      "('chief', 'executive', 'jeff', 'bewkes', 'has', 'however', 'resisted', 'selling', 'in')\n",
      "('stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to')\n",
      "('investors', 'speculated', 'that', 'a', 'deal', 'could', 'spark')\n",
      "('market', 'value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made')\n",
      "('discovery', 'amc', 'netflix', 'and', 'cbs', 'recently')\n",
      "('market', 'value', 'of', 'about', 'SOMENUMbn')\n",
      "('year', 'for', 'SOMENUM.SOMENUMbn')\n",
      "('provider', 'directv')\n",
      "('time', 'warner')\n",
      "('powerhouse', 'buying')\n",
      "('shares', 'including', 'discovery', 'amc', 'netflix')\n",
      "('SOMENUMbn', 'has', 'already', 'made', 'moves')\n",
      "('year',)\n",
      "('SOMENUMbn', 'offer', 'from')\n",
      "('rejected', 'an')\n",
      "('chief', 'executive', 'jeff', 'bewkes', 'has', 'however')\n",
      "('investors', 'speculated', 'that', 'a', 'deal', 'could', 'spark', 'a', 'fresh', 'wave')\n",
      "('negotiations', 'to', 'buy', 'yahoo', 'and', 'has')\n",
      "('negotiations', 'to', 'buy', 'yahoo')\n",
      "('market', 'value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made', 'moves', 'to')\n",
      "('market', 'value', 'of', 'about')\n",
      "('offer', 'from', 'twenty-first', 'century', 'fox', 'inc', 'in')\n",
      "('powerhouse', 'buying', 'satellite', 'tv', 'provider', 'directv', 'last', 'year', 'for')\n",
      "('bought',)\n",
      "('worth', 'SOMENUM.SOMENUMbn')\n",
      "('huffington', 'post')\n",
      "('worth', 'SOMENUM.SOMENUMbn', 'overall')\n",
      "('SOMENUMbn', 'offer', 'from', 'twenty-first')\n",
      "('discovery', 'amc')\n",
      "('SOMENUM.SOMENUMbn', 'overall', 'according', 'to', 'a', 'statement')\n",
      "('deal', 'could', 'spark')\n",
      "('stock',)\n",
      "('value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made')\n",
      "('SOMENUM.SOMENUMbn', 'overall')\n",
      "('deal', 'gives', 'at&t', 'access', 'to', 'a')\n",
      "('completed', 'by', 'the', 'end', 'of')\n",
      "('chief', 'executive', 'jeff', 'bewkes', 'has', 'however', 'resisted', 'selling')\n",
      "('huffington',)\n",
      "('provider', 'directv', 'last')\n",
      "('stock', 'worth', 'SOMENUM.SOMENUMbn')\n",
      "('stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to', 'a')\n",
      "('negotiations', 'to', 'buy', 'yahoo', 'and')\n",
      "('powerhouse', 'buying', 'satellite', 'tv', 'provider', 'directv')\n",
      "('SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox', 'inc', 'in', 'SOMENUM')\n",
      "('deal', 'gives', 'at&t', 'access', 'to', 'a', 'major')\n",
      "('deal',)\n",
      "('SOMENUM.SOMENUMbn', 'overall', 'according', 'to', 'a')\n",
      "('turn', 'itself', 'into', 'a', 'media', 'powerhouse', 'buying', 'satellite', 'tv', 'provider')\n",
      "('negotiations', 'to')\n",
      "('negotiations',)\n",
      "('shares', 'including', 'discovery', 'amc', 'netflix', 'and', 'cbs', 'recently', 'rose', 'as')\n",
      "('bought', 'aol')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer')\n",
      "('deal', 'to', 'be', 'completed', 'by', 'the', 'end')\n",
      "('share', 'in', 'a', 'combination')\n",
      "('deal', 'could', 'spark', 'a', 'fresh', 'wave', 'of')\n",
      "('negotiations', 'to', 'buy', 'yahoo', 'and', 'has', 'already', 'bought', 'aol', 'owner')\n",
      "('provider',)\n",
      "('time', 'warner', 'chief', 'executive', 'jeff')\n",
      "('rejected',)\n",
      "('powerhouse', 'buying', 'satellite')\n",
      "('chief', 'executive', 'jeff', 'bewkes', 'has', 'however', 'resisted', 'selling', 'in', 'the')\n",
      "('moves', 'to', 'turn', 'itself', 'into', 'a')\n",
      "('negotiations', 'to', 'buy', 'yahoo', 'and', 'has', 'already')\n",
      "('discovery', 'amc', 'netflix', 'and', 'cbs', 'recently', 'rose', 'as')\n",
      "('buy', 'yahoo', 'and', 'has', 'already', 'bought', 'aol', 'owner', 'of')\n",
      "('offer', 'from', 'twenty-first', 'century', 'fox')\n",
      "('value',)\n",
      "('core', 'telecoms', 'business')\n",
      "('investors', 'speculated', 'that', 'a', 'deal')\n",
      "('expected', 'to', 'close', 'the', 'deal', 'to', 'be', 'completed', 'by')\n",
      "('investors', 'speculated', 'that', 'a', 'deal', 'could')\n",
      "('powerhouse', 'buying', 'satellite', 'tv')\n",
      "('deal', 'could', 'spark', 'a', 'fresh', 'wave', 'of', 'takeovers', 'and', 'mergers')\n",
      "('value', 'of', 'about', 'SOMENUMbn', 'has')\n",
      "('SOMENUMbn', 'has', 'already', 'made', 'moves', 'to', 'turn')\n",
      "('chief', 'executive', 'jeff', 'bewkes', 'has')\n",
      "('time', 'warner', 'chief')\n",
      "('turn', 'itself', 'into', 'a', 'media', 'powerhouse')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox', 'inc', 'in')\n",
      "('discovery', 'amc', 'netflix', 'and', 'cbs', 'recently', 'rose')\n",
      "('SOMENUMbn', 'has', 'already')\n",
      "('SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox', 'inc', 'in')\n",
      "('market', 'value', 'of', 'about', 'SOMENUMbn', 'has', 'already')\n",
      "('SOMENUMbn', 'offer', 'from', 'twenty-first', 'century')\n",
      "('deal', 'could', 'spark', 'a', 'fresh', 'wave', 'of', 'takeovers', 'and')\n",
      "('combination',)\n",
      "('time', 'warner', 'share', 'in', 'a')\n",
      "('shares', 'including', 'discovery', 'amc')\n",
      "('time',)\n",
      "('stock', 'worth')\n",
      "('offer',)\n",
      "('owner', 'of', 'huffington')\n",
      "('turn',)\n",
      "('combination', 'of', 'cash', 'and', 'stock')\n",
      "('time', 'warner', 'share', 'in')\n",
      "('core', 'telecoms')\n",
      "('deal', 'to')\n",
      "('deal', 'gives', 'at&t')\n",
      "('buy', 'yahoo', 'and', 'has', 'already', 'bought', 'aol', 'owner', 'of', 'huffington')\n",
      "('share', 'in', 'a', 'combination', 'of', 'cash', 'and')\n",
      "('buy', 'yahoo', 'and', 'has', 'already', 'bought', 'aol', 'owner')\n",
      "('buy',)\n",
      "('expected', 'to', 'close', 'the', 'deal')\n",
      "('market', 'value', 'of')\n",
      "('offer', 'from', 'twenty-first', 'century')\n",
      "('shares', 'including')\n",
      "('deal', 'to', 'be')\n",
      "('SOMENUM.SOMENUMbn', 'overall', 'according')\n",
      "('time', 'warner', 'share', 'in', 'a', 'combination', 'of', 'cash', 'and')\n",
      "('moves', 'to')\n",
      "('SOMENUMbn', 'has')\n",
      "('deal', 'gives', 'at&t', 'access', 'to', 'a', 'major', 'producer', 'of')\n",
      "('deal', 'could', 'spark', 'a', 'fresh', 'wave')\n",
      "('combination', 'of')\n",
      "('stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to', 'a', 'statement')\n",
      "('chief', 'executive')\n",
      "('owner', 'of')\n",
      "('discovery', 'amc', 'netflix', 'and', 'cbs')\n",
      "('chief', 'executive', 'jeff')\n",
      "('twenty-first',)\n",
      "('SOMENUM.SOMENUMbn', 'overall', 'according', 'to')\n",
      "('deal', 'could')\n",
      "('deal', 'gives')\n",
      "('combination', 'of', 'cash', 'and', 'stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall')\n",
      "('deal', 'could', 'spark', 'a', 'fresh')\n",
      "('time', 'warner', 'share')\n",
      "('worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to', 'a', 'statement')\n",
      "('share', 'in', 'a', 'combination', 'of', 'cash', 'and', 'stock')\n",
      "('takeover',)\n",
      "('expected', 'to', 'close', 'the', 'deal', 'to')\n",
      "('pay',)\n",
      "('twenty-first', 'century', 'fox', 'inc', 'in', 'SOMENUM')\n",
      "('share', 'in', 'a', 'combination', 'of')\n",
      "('SOMENUMbn', 'has', 'already', 'made', 'moves', 'to')\n",
      "('discovery',)\n",
      "('SOMENUM.SOMENUMbn',)\n",
      "('powerhouse', 'buying', 'satellite', 'tv', 'provider', 'directv', 'last', 'year')\n",
      "('core',)\n",
      "('completed', 'by')\n",
      "('completed', 'by', 'the')\n",
      "('market', 'value')\n",
      "('share',)\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each', 'time', 'warner', 'share', 'in')\n",
      "('time', 'warner', 'share', 'in', 'a', 'combination')\n",
      "('market', 'value', 'of', 'about', 'SOMENUMbn', 'has')\n",
      "('shares', 'including', 'discovery', 'amc', 'netflix', 'and', 'cbs', 'recently', 'rose')\n",
      "('bought', 'aol', 'owner', 'of', 'huffington')\n",
      "('deal', 'to', 'be', 'completed', 'by', 'the')\n",
      "('share', 'in', 'a', 'combination', 'of', 'cash', 'and', 'stock', 'worth')\n",
      "('year', 'for')\n",
      "('shares', 'including', 'discovery', 'amc', 'netflix', 'and', 'cbs')\n",
      "('chief', 'executive', 'jeff', 'bewkes')\n",
      "('worth',)\n",
      "('moves', 'to', 'turn', 'itself', 'into', 'a', 'media', 'powerhouse', 'buying')\n",
      "('combination', 'of', 'cash', 'and')\n",
      "('end',)\n",
      "('turn', 'itself', 'into', 'a', 'media', 'powerhouse', 'buying')\n",
      "('buy', 'yahoo', 'and', 'has', 'already')\n",
      "('deal', 'to', 'be', 'completed', 'by')\n",
      "('SOMENUMbn',)\n",
      "('negotiations', 'to', 'buy', 'yahoo', 'and', 'has', 'already', 'bought')\n",
      "('worth', 'SOMENUM.SOMENUMbn', 'overall', 'according')\n",
      "('chief',)\n",
      "('time', 'warner', 'chief', 'executive', 'jeff', 'bewkes', 'has')\n",
      "('SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox')\n",
      "('expected', 'to', 'close', 'the')\n",
      "('investors',)\n",
      "('twenty-first', 'century', 'fox')\n",
      "('time', 'warner', 'chief', 'executive', 'jeff', 'bewkes', 'has', 'however', 'resisted', 'selling')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for')\n",
      "('turn', 'itself', 'into', 'a', 'media', 'powerhouse', 'buying', 'satellite')\n",
      "('buy', 'yahoo', 'and', 'has', 'already', 'bought', 'aol')\n",
      "('completed',)\n",
      "('SOMENUMbn', 'has', 'already', 'made', 'moves', 'to', 'turn', 'itself', 'into', 'a')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each', 'time', 'warner', 'share', 'in', 'a')\n",
      "('bought', 'aol', 'owner', 'of')\n",
      "('moves',)\n",
      "('moves', 'to', 'turn', 'itself', 'into', 'a', 'media', 'powerhouse')\n",
      "('shares',)\n",
      "('shares', 'including', 'discovery', 'amc', 'netflix', 'and')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer', 'from', 'twenty-first', 'century')\n",
      "('offer', 'from', 'twenty-first', 'century', 'fox', 'inc', 'in', 'SOMENUM')\n",
      "('companies',)\n",
      "('end', 'of')\n",
      "('bought', 'aol', 'owner')\n",
      "('powerhouse', 'buying', 'satellite', 'tv', 'provider', 'directv', 'last')\n",
      "('discovery', 'amc', 'netflix', 'and')\n",
      "('buy', 'yahoo')\n",
      "('deal', 'could', 'spark', 'a')\n",
      "('turn', 'itself', 'into', 'a')\n",
      "('deal', 'gives', 'at&t', 'access', 'to', 'a', 'major', 'producer', 'of', 'content')\n",
      "('combination', 'of', 'cash', 'and', 'stock', 'worth')\n",
      "('powerhouse',)\n",
      "('SOMENUMbn', 'has', 'already', 'made', 'moves', 'to', 'turn', 'itself')\n",
      "('value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made', 'moves')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox')\n",
      "('SOMENUMbn', 'has', 'already', 'made')\n",
      "('deal', 'gives', 'at&t', 'access')\n",
      "('discovery', 'amc', 'netflix', 'and', 'cbs', 'recently', 'rose', 'as', 'investors')\n",
      "('powerhouse', 'buying', 'satellite', 'tv', 'provider')\n",
      "('combination', 'of', 'cash', 'and', 'stock', 'worth', 'SOMENUM.SOMENUMbn', 'overall', 'according')\n",
      "('negotiations', 'to', 'buy', 'yahoo', 'and', 'has', 'already', 'bought', 'aol')\n",
      "('pay', 'SOMENUM.SOMENUM')\n",
      "('value', 'of')\n",
      "('shares', 'including', 'discovery', 'amc', 'netflix', 'and', 'cbs', 'recently')\n",
      "('expected', 'to')\n",
      "('time', 'warner', 'share', 'in', 'a', 'combination', 'of', 'cash')\n",
      "('share', 'in')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer', 'from', 'twenty-first')\n",
      "('turn', 'itself')\n",
      "('SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox', 'inc')\n",
      "('time', 'warner', 'chief', 'executive', 'jeff', 'bewkes', 'has', 'however')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each', 'time', 'warner', 'share')\n",
      "('time', 'warner', 'share', 'in', 'a', 'combination', 'of')\n",
      "('time', 'warner', 'chief', 'executive', 'jeff', 'bewkes', 'has', 'however', 'resisted')\n",
      "('provider', 'directv', 'last', 'year')\n",
      "('market', 'value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made', 'moves')\n",
      "('share', 'in', 'a', 'combination', 'of', 'cash')\n",
      "('shares', 'including', 'discovery')\n",
      "('offer', 'from')\n",
      "('chief', 'executive', 'jeff', 'bewkes', 'has', 'however', 'resisted')\n",
      "('deal', 'to', 'be', 'completed', 'by', 'the', 'end', 'of', 'SOMENUM')\n",
      "('moves', 'to', 'turn', 'itself')\n",
      "('rejected', 'an', 'SOMENUMbn', 'offer', 'from', 'twenty-first', 'century', 'fox', 'inc')\n",
      "('SOMENUMbn', 'offer')\n",
      "('value', 'of', 'about', 'SOMENUMbn', 'has', 'already')\n",
      "('twenty-first', 'century', 'fox', 'inc', 'in')\n",
      "('moves', 'to', 'turn')\n",
      "('expected', 'to', 'close', 'the', 'deal', 'to', 'be')\n",
      "('takeover', 'of', 'time', 'warner')\n",
      "('owner',)\n",
      "('time', 'warner', 'chief', 'executive')\n",
      "('expected', 'to', 'close', 'the', 'deal', 'to', 'be', 'completed', 'by', 'the')\n",
      "('investors', 'speculated', 'that', 'a', 'deal', 'could', 'spark', 'a')\n",
      "('deal', 'gives', 'at&t', 'access', 'to')\n",
      "('provider', 'directv', 'last', 'year', 'for', 'SOMENUM.SOMENUMbn')\n",
      "('owner', 'of', 'huffington', 'post')\n",
      "('value', 'of', 'about', 'SOMENUMbn', 'has', 'already', 'made', 'moves', 'to', 'turn')\n",
      "('buy', 'yahoo', 'and')\n",
      "('moves', 'to', 'turn', 'itself', 'into', 'a', 'media', 'powerhouse', 'buying', 'satellite')\n",
      "('pay', 'SOMENUM.SOMENUM', 'for', 'each', 'time', 'warner', 'share', 'in', 'a', 'combination')\n",
      "('worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to', 'a')\n",
      "('business',)\n",
      "('buy', 'yahoo', 'and', 'has')\n",
      "('worth', 'SOMENUM.SOMENUMbn', 'overall', 'according', 'to')\n"
     ]
    }
   ],
   "source": [
    "bold_range = np.zeros(len(new_text))\n",
    "bold_range.shape\n",
    "for gram in ngram_of_article:\n",
    "    if ngram_of_article[gram][2]==1:\n",
    "        print(gram)\n",
    "        #print(ngram_of_article[gram][0])\n",
    "        for span in ngram_of_article[gram][0]:\n",
    "            bold_range[span[0]:span[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bold_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bold_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "AT&T will <b>pay $107.50 for each Time Warner share, in a combination of cash and stock, worth $85.4bn overall, according to a statement.</b>\n",
       "AT&T said it <b>expected to close the deal to be completed by the end of 2017.</b>\n",
       "Other media company <b>shares, including Discovery, AMC, Netflix and CBS, recently rose as investors speculated that a deal could spark a fresh wave of takeovers and mergers</b> among media and technology <b>companies.</b>\n",
       "AT&T, which has a <b>market value of about $238bn, has already made moves to turn itself into a media powerhouse, buying satellite TV provider DirecTV last year for $48.5bn.</b>\n",
       "<b>Time Warner chief executive Jeff Bewkes has, however, resisted selling in the</b> <b>past.</b> The company <b>rejected an $80bn offer from Twenty-First Century Fox Inc in 2014.</b>\n",
       "The <b>deal gives AT&T access to a major producer of content</b> as it seeks to diversify away from its <b>core telecoms business.</b> Rival Verizon is currently in <b>negotiations to buy Yahoo and has already bought AOL, owner of Huffington Post.</b>\n",
       "Some analysts, however, question whether AT&T needs to mount a complete <b>takeover of Time Warner.</b>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "html_string = \"\"\n",
    "temp_bold = ''\n",
    "for idx,i in enumerate(bold_range):\n",
    "    if i==1:\n",
    "        temp_bold += new_text[idx]\n",
    "    else:\n",
    "        if temp_bold:\n",
    "            html_string += \"<b>{}</b>\".format(temp_bold)\n",
    "            temp_bold = \"\"\n",
    "        html_string += new_text[idx]\n",
    "display(HTML(html_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<b>South Korea registered a trade deficit of $101 million in October, reflecting the country's</b> <b>economic sluggishness, according to government figures released Wednesday.</b> Preliminary tallies by the <b>Trade and Industry Ministry showed another trade deficit in October, the fifth monthly setback this year, casting a cloud on South Korea's export-oriented economy.</b> Exports in October <b>stood at $5.29 billion, a mere 0.7% increase from a</b> <b>year earlier, while imports increased sharply to $5.39 billion, up</b> 20% from last October. <b>South Korea's economic boom, which began in 1986, stopped this year because of prolonged labor disputes, trade conflicts and sluggish exports.</b> <b>Government officials said exports at the end of the year would remain under a government target of $68 billion.</b> Despite the gloomy forecast, <b>South Korea has recorded a trade surplus of $71 million so far this year.</b> From January to October, the nation's <b>accumulated exports increased 4% from the same period last year to $50.45 billion.</b> Imports were at $50.38 billion, up 19%.\\n\\n\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407786, 2)\n",
      "BUSINESS\n",
      "0    319073\n",
      "1     88713\n",
      "Name: BUSINESS, dtype: int64\n",
      "Train-\n",
      "BUSINESS\n",
      "0    62099\n",
      "1    62099\n",
      "Name: BUSINESS, dtype: int64\n",
      "Validation-\n",
      "BUSINESS\n",
      "0    8872\n",
      "1    8872\n",
      "Name: BUSINESS, dtype: int64\n",
      "Test-\n",
      "BUSINESS\n",
      "0    17742\n",
      "1    17742\n",
      "Name: BUSINESS, dtype: int64\n",
      "number of samples - 124198\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 30735937039.265110016 training accuracy= 0.6617658899499187 validation accuracy= 0.6601104598737602\n",
      "saving new best validation model - 0.6601104598737602\n",
      "Epoch: 0002 cost= 16694745769.566478729 training accuracy= 0.6952044316333597 validation accuracy= 0.6946009918845807\n",
      "saving new best validation model - 0.6946009918845807\n",
      "Epoch: 0003 cost= 11850029181.421434402 training accuracy= 0.7112513889112546 validation accuracy= 0.7067741208295762\n",
      "saving new best validation model - 0.7067741208295762\n",
      "Epoch: 0004 cost= 9006856486.575342178 training accuracy= 0.7240132691347687 validation accuracy= 0.717143823264202\n",
      "saving new best validation model - 0.717143823264202\n",
      "Epoch: 0005 cost= 7056882232.109588623 training accuracy= 0.7338523969790174 validation accuracy= 0.7252592425608656\n",
      "saving new best validation model - 0.7252592425608656\n",
      "Epoch: 0006 cost= 5622209560.651087761 training accuracy= 0.7425723441601314 validation accuracy= 0.731965734896303\n",
      "saving new best validation model - 0.731965734896303\n",
      "Epoch: 0007 cost= 4518506586.352941513 training accuracy= 0.7507930884555307 validation accuracy= 0.7377705139765555\n",
      "saving new best validation model - 0.7377705139765555\n",
      "Epoch: 0008 cost= 3645559754.365834236 training accuracy= 0.7576047923477028 validation accuracy= 0.7416027953110911\n",
      "saving new best validation model - 0.7416027953110911\n",
      "Epoch: 0009 cost= 2949789621.221595287 training accuracy= 0.7635791236573858 validation accuracy= 0.7459422903516681\n",
      "saving new best validation model - 0.7459422903516681\n",
      "Epoch: 0010 cost= 2388550391.903303623 training accuracy= 0.7701009678094656 validation accuracy= 0.7502254283137962\n",
      "saving new best validation model - 0.7502254283137962\n",
      "Epoch: 0011 cost= 1934370191.239323139 training accuracy= 0.7755197346173046 validation accuracy= 0.7536632100991885\n",
      "saving new best validation model - 0.7536632100991885\n",
      "Epoch: 0012 cost= 1567060903.529411793 training accuracy= 0.7813088777597063 validation accuracy= 0.7561429215509468\n",
      "saving new best validation model - 0.7561429215509468\n",
      "Epoch: 0013 cost= 1270307022.478646278 training accuracy= 0.7865102497624761 validation accuracy= 0.7592989179440938\n",
      "saving new best validation model - 0.7592989179440938\n",
      "Epoch: 0014 cost= 1029256115.597099066 training accuracy= 0.7908178875666275 validation accuracy= 0.7615532010820559\n",
      "saving new best validation model - 0.7615532010820559\n",
      "Epoch: 0015 cost= 832065193.050765514 training accuracy= 0.7949966988196268 validation accuracy= 0.7640892696122633\n",
      "saving new best validation model - 0.7640892696122633\n",
      "Epoch: 0016 cost= 671240442.829975843 training accuracy= 0.7974605066104125 validation accuracy= 0.7642019837691614\n",
      "saving new best validation model - 0.7642019837691614\n",
      "Epoch: 0017 cost= 540690112.509266734 training accuracy= 0.8037166460007408 validation accuracy= 0.7671325518485121\n",
      "saving new best validation model - 0.7671325518485121\n",
      "Epoch: 0018 cost= 433211669.653505266 training accuracy= 0.8060757822187152 validation accuracy= 0.7696122633002705\n",
      "saving new best validation model - 0.7696122633002705\n",
      "Epoch: 0019 cost= 344648601.037872672 training accuracy= 0.8108826229085815 validation accuracy= 0.7704012623985572\n",
      "saving new best validation model - 0.7704012623985572\n",
      "Epoch: 0020 cost= 272613029.434327185 training accuracy= 0.8147796260809352 validation accuracy= 0.7749098286744815\n",
      "saving new best validation model - 0.7749098286744815\n",
      "Epoch: 0021 cost= 215489434.067687362 training accuracy= 0.8171226589800158 validation accuracy= 0.7741771866546439\n",
      "reduced validation acc!\n",
      "Epoch: 0022 cost= 168090905.292506039 training accuracy= 0.8201983928887744 validation accuracy= 0.7776149684400361\n",
      "saving new best validation model - 0.7776149684400361\n",
      "Epoch: 0023 cost= 130128333.767929092 training accuracy= 0.8189423340150405 validation accuracy= 0.7774458972046889\n",
      "reduced validation acc!\n",
      "Epoch: 0024 cost= 100104339.072522163 training accuracy= 0.8228715438251823 validation accuracy= 0.7792493237150586\n",
      "saving new best validation model - 0.7792493237150586\n",
      "Epoch: 0025 cost= 76103997.497985497 training accuracy= 0.7974041449942834 validation accuracy= 0.7663999098286745\n",
      "reduced validation acc!\n",
      "Epoch: 0026 cost= 56772023.543916196 training accuracy= 0.8228715438251823 validation accuracy= 0.780714607754734\n",
      "saving new best validation model - 0.780714607754734\n",
      "Epoch: 0027 cost= 43556006.539081387 training accuracy= 0.8211806953413099 validation accuracy= 0.7833070333633905\n",
      "saving new best validation model - 0.7833070333633905\n",
      "Epoch: 0028 cost= 33784869.564867042 training accuracy= 0.819972946424258 validation accuracy= 0.7749661857529305\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 26676434.465753425 training accuracy= 0.8287412035620542 validation accuracy= 0.787082957619477\n",
      "saving new best validation model - 0.787082957619477\n",
      "Epoch: 0030 cost= 22908935.322723608 training accuracy= 0.8304723103431617 validation accuracy= 0.7897317403065826\n",
      "saving new best validation model - 0.7897317403065826\n",
      "Epoch: 0031 cost= 20457875.272763900 training accuracy= 0.8038535242113399 validation accuracy= 0.7657236248872858\n",
      "reduced validation acc!\n",
      "Epoch: 0032 cost= 17821966.493956488 training accuracy= 0.8045701219021241 validation accuracy= 0.7648782687105501\n",
      "reduced validation acc!\n",
      "Epoch: 0033 cost= 16278849.319500403 training accuracy= 0.8277025394933896 validation accuracy= 0.7861812443642922\n",
      "saving new best validation model - 0.7861812443642922\n",
      "Epoch: 0034 cost= 17673626.188759066 training accuracy= 0.8222354627288684 validation accuracy= 0.7803764652840397\n",
      "reduced validation acc!\n",
      "Epoch: 0035 cost= 17134838.417002417 training accuracy= 0.8314868194334852 validation accuracy= 0.7843778178539225\n",
      "saving new best validation model - 0.7843778178539225\n",
      "Epoch: 0036 cost= 16338865.270950846 training accuracy= 0.8225977874039839 validation accuracy= 0.7825743913435528\n",
      "reduced validation acc!\n",
      "Epoch: 0037 cost= 17048155.928887993 training accuracy= 0.8429765374643714 validation accuracy= 0.7920423805229937\n",
      "saving new best validation model - 0.7920423805229937\n",
      "Epoch: 0038 cost= 17286668.067687348 training accuracy= 0.7948598206090275 validation accuracy= 0.7674143372407575\n",
      "reduced validation acc!\n",
      "Epoch: 0039 cost= 16378148.151188558 training accuracy= 0.8314224061579092 validation accuracy= 0.7877592425608656\n",
      "saving new best validation model - 0.7877592425608656\n",
      "Epoch: 0040 cost= 16231299.715350524 training accuracy= 0.801228683231614 validation accuracy= 0.7707957619477006\n",
      "reduced validation acc!\n",
      "Epoch: 0041 cost= 15416769.798348106 training accuracy= 0.8117844087666468 validation accuracy= 0.7781221821460775\n",
      "saving new best validation model - 0.7781221821460775\n",
      "Epoch: 0042 cost= 16330979.359589040 training accuracy= 0.836615726501232 validation accuracy= 0.7950856627592425\n",
      "saving new best validation model - 0.7950856627592425\n",
      "Epoch: 0043 cost= 18143407.807010476 training accuracy= 0.8138939435417639 validation accuracy= 0.7817290351668169\n",
      "reduced validation acc!\n",
      "Epoch: 0044 cost= 17056455.426974215 training accuracy= 0.8117280471505177 validation accuracy= 0.7820108205590622\n",
      "saving new best validation model - 0.7820108205590622\n",
      "Epoch: 0045 cost= 16088927.874294924 training accuracy= 0.7972994734214721 validation accuracy= 0.7688232642019838\n",
      "reduced validation acc!\n",
      "Epoch: 0046 cost= 16501961.056809025 training accuracy= 0.8623327267749883 validation accuracy= 0.8112037871956718\n",
      "saving new best validation model - 0.8112037871956718\n",
      "Epoch: 0047 cost= 15023470.344505439 training accuracy= 0.8258345545016827 validation accuracy= 0.789168169522092\n",
      "reduced validation acc!\n",
      "Epoch: 0048 cost= 14983479.343523368 training accuracy= 0.8591040113367365 validation accuracy= 0.8081041478809738\n",
      "saving new best validation model - 0.8081041478809738\n",
      "Epoch: 0049 cost= 14911684.923851732 training accuracy= 0.8651347042625485 validation accuracy= 0.8094003606853021\n",
      "saving new best validation model - 0.8094003606853021\n",
      "Epoch: 0050 cost= 15114662.500705076 training accuracy= 0.8587175316832799 validation accuracy= 0.8002705139765555\n",
      "reduced validation acc!\n",
      "Epoch: 0051 cost= 16734979.356970185 training accuracy= 0.8540475692040129 validation accuracy= 0.7951420198376916\n",
      "reduced validation acc!\n",
      "Epoch: 0052 cost= 16676803.309528606 training accuracy= 0.8683875746791414 validation accuracy= 0.8064697926059513\n",
      "saving new best validation model - 0.8064697926059513\n",
      "Epoch: 0053 cost= 15067286.652699435 training accuracy= 0.8570186315399604 validation accuracy= 0.7964945897204689\n",
      "reduced validation acc!\n",
      "Epoch: 0054 cost= 15059243.203616036 training accuracy= 0.8544904104735986 validation accuracy= 0.7918733092876465\n",
      "reduced validation acc!\n",
      "Epoch: 0055 cost= 14634745.146958098 training accuracy= 0.8732507769851366 validation accuracy= 0.8071460775473399\n",
      "saving new best validation model - 0.8071460775473399\n",
      "Epoch: 0056 cost= 15710877.557715552 training accuracy= 0.8792492632731606 validation accuracy= 0.8128381424706943\n",
      "saving new best validation model - 0.8128381424706943\n",
      "Epoch: 0057 cost= 16406161.352941176 training accuracy= 0.876817662120163 validation accuracy= 0.8200518485121732\n",
      "saving new best validation model - 0.8200518485121732\n",
      "Epoch: 0058 cost= 15971516.240330379 training accuracy= 0.8108423646113464 validation accuracy= 0.7780658250676284\n",
      "reduced validation acc!\n",
      "Epoch: 0059 cost= 16051970.532987511 training accuracy= 0.8439346849385658 validation accuracy= 0.8019612263300271\n",
      "saving new best validation model - 0.8019612263300271\n",
      "Epoch: 0060 cost= 15768169.206914786 training accuracy= 0.8869466497045041 validation accuracy= 0.8181357078449053\n",
      "saving new best validation model - 0.8181357078449053\n",
      "Epoch: 0061 cost= 15701048.727009468 training accuracy= 0.880175204109567 validation accuracy= 0.8179666366095582\n",
      "reduced validation acc!\n",
      "Epoch: 0062 cost= 15573717.179290894 training accuracy= 0.8412454306832639 validation accuracy= 0.7971708746618575\n",
      "reduced validation acc!\n",
      "Epoch: 0063 cost= 15404091.298952458 training accuracy= 0.8554485579477931 validation accuracy= 0.8033137962128043\n",
      "saving new best validation model - 0.8033137962128043\n",
      "Epoch: 0064 cost= 15090164.391695205 training accuracy= 0.8365352099067618 validation accuracy= 0.7928313796212805\n",
      "reduced validation acc!\n",
      "Epoch: 0065 cost= 15310325.157433521 training accuracy= 0.8305689302565259 validation accuracy= 0.7877592425608656\n",
      "reduced validation acc!\n",
      "Epoch: 0066 cost= 14851982.768734891 training accuracy= 0.8285882220325609 validation accuracy= 0.789168169522092\n",
      "saving new best validation model - 0.789168169522092\n",
      "Epoch: 0067 cost= 14609784.579321112 training accuracy= 0.8189745406528286 validation accuracy= 0.7811091073038774\n",
      "reduced validation acc!\n",
      "Epoch: 0068 cost= 15399313.929492345 training accuracy= 0.8046828451343822 validation accuracy= 0.7728809738503156\n",
      "reduced validation acc!\n",
      "Epoch: 0069 cost= 13996359.245807312 training accuracy= 0.8607143432261389 validation accuracy= 0.8088931469792606\n",
      "saving new best validation model - 0.8088931469792606\n",
      "Epoch: 0070 cost= 12713898.585824184 training accuracy= 0.86416045346946 validation accuracy= 0.8081605049594229\n",
      "reduced validation acc!\n",
      "Epoch: 0071 cost= 13705501.588789282 training accuracy= 0.8429604341454774 validation accuracy= 0.7971708746618575\n",
      "reduced validation acc!\n",
      "Epoch: 0072 cost= 14484389.541372884 training accuracy= 0.8904088632667193 validation accuracy= 0.8076532912533815\n",
      "saving new best validation model - 0.8076532912533815\n",
      "Epoch: 0073 cost= 13649097.272411361 training accuracy= 0.9090726098648931 validation accuracy= 0.8179102795311091\n",
      "saving new best validation model - 0.8179102795311091\n",
      "Epoch: 0074 cost= 12655979.009707393 training accuracy= 0.9075750012077489 validation accuracy= 0.8160504959422904\n",
      "reduced validation acc!\n",
      "Epoch: 0075 cost= 12044058.746348711 training accuracy= 0.9004734375754843 validation accuracy= 0.8111474301172227\n",
      "reduced validation acc!\n",
      "Epoch: 0076 cost= 11626391.964569904 training accuracy= 0.9024299908211082 validation accuracy= 0.8096257889990983\n",
      "reduced validation acc!\n",
      "Epoch: 0077 cost= 12048302.686423499 training accuracy= 0.8516803813265914 validation accuracy= 0.7799256086564472\n",
      "reduced validation acc!\n",
      "Epoch: 0078 cost= 13898410.961236527 training accuracy= 0.9190888742169762 validation accuracy= 0.8203336339044184\n",
      "saving new best validation model - 0.8203336339044184\n",
      "Epoch: 0079 cost= 14136054.634594077 training accuracy= 0.9194511988920917 validation accuracy= 0.8301961226330027\n",
      "saving new best validation model - 0.8301961226330027\n",
      "Epoch: 0080 cost= 13731810.164106566 training accuracy= 0.9319715293321954 validation accuracy= 0.8286181244364292\n",
      "reduced validation acc!\n",
      "Epoch: 0081 cost= 14179829.109875478 training accuracy= 0.9334127763732105 validation accuracy= 0.828392696122633\n",
      "reduced validation acc!\n",
      "Epoch: 0082 cost= 13060151.120719178 training accuracy= 0.9314884297653746 validation accuracy= 0.823095130748422\n",
      "reduced validation acc!\n",
      "Epoch: 0083 cost= 12678404.447509568 training accuracy= 0.9213916488188215 validation accuracy= 0.8163322813345356\n",
      "reduced validation acc!\n",
      "Epoch: 0084 cost= 12674885.015879370 training accuracy= 0.9361100822879596 validation accuracy= 0.8263638412984671\n",
      "saving new best validation model - 0.8263638412984671\n",
      "Epoch: 0085 cost= 12398787.443896051 training accuracy= 0.9327364369796615 validation accuracy= 0.8207844905320109\n",
      "reduced validation acc!\n",
      "Epoch: 0086 cost= 12350764.256414937 training accuracy= 0.9327203336607675 validation accuracy= 0.8216298467087466\n",
      "saving new best validation model - 0.8216298467087466\n",
      "Epoch: 0087 cost= 12392955.884150257 training accuracy= 0.9389523180727548 validation accuracy= 0.825236699729486\n",
      "saving new best validation model - 0.825236699729486\n",
      "Epoch: 0088 cost= 13069415.884880515 training accuracy= 0.943799417059856 validation accuracy= 0.8281109107303878\n",
      "saving new best validation model - 0.8281109107303878\n",
      "Epoch: 0089 cost= 12927746.771084622 training accuracy= 0.9454580589059405 validation accuracy= 0.8296889089269612\n",
      "saving new best validation model - 0.8296889089269612\n",
      "Epoch: 0090 cost= 12002965.185550023 training accuracy= 0.941738192241421 validation accuracy= 0.831605049594229\n",
      "saving new best validation model - 0.831605049594229\n",
      "Epoch: 0091 cost= 11215338.353526643 training accuracy= 0.9427527013317445 validation accuracy= 0.8319995491433724\n",
      "saving new best validation model - 0.8319995491433724\n",
      "Epoch: 0092 cost= 10644766.997698272 training accuracy= 0.9374305544372695 validation accuracy= 0.8307596934174932\n",
      "reduced validation acc!\n",
      "Epoch: 0093 cost= 9934160.874957802 training accuracy= 0.9448300294690736 validation accuracy= 0.8312105500450857\n",
      "saving new best validation model - 0.8312105500450857\n",
      "Epoch: 0094 cost= 9558677.901676051 training accuracy= 0.9428412695856616 validation accuracy= 0.8322249774571686\n",
      "saving new best validation model - 0.8322249774571686\n",
      "Epoch: 0095 cost= 9301497.559695432 training accuracy= 0.9191130291953171 validation accuracy= 0.825180342651037\n",
      "reduced validation acc!\n",
      "Epoch: 0096 cost= 10421749.378537973 training accuracy= 0.9551361535612489 validation accuracy= 0.8312669071235347\n",
      "saving new best validation model - 0.8312669071235347\n",
      "Epoch: 0097 cost= 10860728.844598295 training accuracy= 0.9422776534243708 validation accuracy= 0.8189810640216412\n",
      "reduced validation acc!\n",
      "Epoch: 0098 cost= 9953345.628063463 training accuracy= 0.9482519847340537 validation accuracy= 0.8229260595130748\n",
      "saving new best validation model - 0.8229260595130748\n",
      "Epoch: 0099 cost= 10019756.773865730 training accuracy= 0.9418750704520201 validation accuracy= 0.8201645626690712\n",
      "reduced validation acc!\n",
      "Epoch: 0100 cost= 10074772.244480573 training accuracy= 0.9566257105589462 validation accuracy= 0.8257439134355276\n",
      "saving new best validation model - 0.8257439134355276\n",
      "Epoch: 0101 cost= 9895806.074113304 training accuracy= 0.9526079324948872 validation accuracy= 0.825236699729486\n",
      "reduced validation acc!\n",
      "Epoch: 0102 cost= 9815228.744469555 training accuracy= 0.943106974347413 validation accuracy= 0.8187556357078449\n",
      "reduced validation acc!\n",
      "Epoch: 0103 cost= 9945977.473395951 training accuracy= 0.9536224415852107 validation accuracy= 0.8248985572587917\n",
      "saving new best validation model - 0.8248985572587917\n",
      "Epoch: 0104 cost= 9407009.449255735 training accuracy= 0.9496610251372808 validation accuracy= 0.8221370604147881\n",
      "reduced validation acc!\n",
      "Epoch: 0105 cost= 8463082.946163943 training accuracy= 0.956504935667241 validation accuracy= 0.8243913435527502\n",
      "saving new best validation model - 0.8243913435527502\n",
      "Epoch: 0106 cost= 7861322.374358074 training accuracy= 0.9618834441778451 validation accuracy= 0.8270401262398557\n",
      "saving new best validation model - 0.8270401262398557\n",
      "Epoch: 0107 cost= 7549850.553522077 training accuracy= 0.9686226831349941 validation accuracy= 0.8303088367899009\n",
      "saving new best validation model - 0.8303088367899009\n",
      "Epoch: 0108 cost= 7263267.111189628 training accuracy= 0.9707563728884523 validation accuracy= 0.8329012623985572\n",
      "saving new best validation model - 0.8329012623985572\n",
      "Epoch: 0109 cost= 7236504.415308778 training accuracy= 0.9698143287331519 validation accuracy= 0.8349864743011722\n",
      "saving new best validation model - 0.8349864743011722\n",
      "Epoch: 0110 cost= 7078403.018860797 training accuracy= 0.9700639301760092 validation accuracy= 0.8285054102795311\n",
      "reduced validation acc!\n",
      "Epoch: 0111 cost= 7307764.974520798 training accuracy= 0.9721090516755503 validation accuracy= 0.8327321911632101\n",
      "saving new best validation model - 0.8327321911632101\n",
      "Epoch: 0112 cost= 7631748.535348257 training accuracy= 0.9582038358105606 validation accuracy= 0.8220807033363391\n",
      "reduced validation acc!\n",
      "Epoch: 0113 cost= 8548046.710503122 training accuracy= 0.9423259633810528 validation accuracy= 0.8132889990982868\n",
      "reduced validation acc!\n",
      "Epoch: 0114 cost= 8428715.153114337 training accuracy= 0.9623021304690896 validation accuracy= 0.8229260595130748\n",
      "saving new best validation model - 0.8229260595130748\n",
      "Epoch: 0115 cost= 7309889.100428259 training accuracy= 0.9789690655244045 validation accuracy= 0.8319431920649234\n",
      "saving new best validation model - 0.8319431920649234\n",
      "Epoch: 0116 cost= 6688595.458111882 training accuracy= 0.9806438106893831 validation accuracy= 0.8348174030658251\n",
      "saving new best validation model - 0.8348174030658251\n",
      "Epoch: 0117 cost= 6034527.972871471 training accuracy= 0.9817790946714118 validation accuracy= 0.8348174030658251\n",
      "reduced validation acc!\n",
      "Epoch: 0118 cost= 5875664.982114535 training accuracy= 0.9817790946714118 validation accuracy= 0.8345919747520288\n",
      "reduced validation acc!\n",
      "Epoch: 0119 cost= 5019899.568271240 training accuracy= 0.981263788466803 validation accuracy= 0.8318868349864743\n",
      "reduced validation acc!\n",
      "Epoch: 0120 cost= 4792553.502891325 training accuracy= 0.9834940981336253 validation accuracy= 0.8345919747520288\n",
      "saving new best validation model - 0.8345919747520288\n",
      "Epoch: 0121 cost= 4347766.981262281 training accuracy= 0.983904732765423 validation accuracy= 0.8350991884580703\n",
      "saving new best validation model - 0.8350991884580703\n",
      "Epoch: 0122 cost= 4231069.738514178 training accuracy= 0.9818032496497529 validation accuracy= 0.8375225428313796\n",
      "saving new best validation model - 0.8375225428313796\n",
      "Epoch: 0123 cost= 3977725.042035861 training accuracy= 0.9703940482133367 validation accuracy= 0.8371844003606853\n",
      "reduced validation acc!\n",
      "Epoch: 0124 cost= 4031061.585593618 training accuracy= 0.9677128456174817 validation accuracy= 0.8361136158701533\n",
      "reduced validation acc!\n",
      "Epoch: 0125 cost= 3799136.201947380 training accuracy= 0.9764247411391488 validation accuracy= 0.8360009017132551\n",
      "reduced validation acc!\n",
      "Epoch: 0126 cost= 3691546.423444503 training accuracy= 0.9727451327718643 validation accuracy= 0.8371844003606853\n",
      "saving new best validation model - 0.8371844003606853\n",
      "Epoch: 0127 cost= 3568367.804005047 training accuracy= 0.9836551313225655 validation accuracy= 0.8378606853020739\n",
      "saving new best validation model - 0.8378606853020739\n",
      "Epoch: 0128 cost= 3294303.653958600 training accuracy= 0.9822944008760206 validation accuracy= 0.8349864743011722\n",
      "reduced validation acc!\n",
      "Epoch: 0129 cost= 2882779.804168922 training accuracy= 0.9880513373806341 validation accuracy= 0.8356064021641119\n",
      "saving new best validation model - 0.8356064021641119\n",
      "Epoch: 0130 cost= 2650735.252752238 training accuracy= 0.9881560089534454 validation accuracy= 0.8348737601442742\n",
      "reduced validation acc!\n",
      "Epoch: 0131 cost= 2520686.843764164 training accuracy= 0.9893798611893911 validation accuracy= 0.8350991884580703\n",
      "saving new best validation model - 0.8350991884580703\n",
      "Epoch: 0132 cost= 2388442.005441720 training accuracy= 0.9894523261244142 validation accuracy= 0.8362263300270514\n",
      "saving new best validation model - 0.8362263300270514\n"
     ]
    }
   ],
   "source": [
    "for i in [3,4,5,6,7,8,9]:\n",
    "    train_validation_test = split_test_validation('ngram_10krows_word2vec_0{}_threshold.pickle'.format(i))\n",
    "    for n_layer in [3]:\n",
    "        \n",
    "        epoch_values=[]\n",
    "        cost_values=[]\n",
    "        training_accuracy_values=[]\n",
    "        validation_accuracy_values=[]\n",
    "        update(epoch_values,cost_values,training_accuracy_values,validation_accuracy_values)\n",
    "        model_name = 'word2vec_model_0{}_layers{}'.format(i, n_layer)\n",
    "\n",
    "        run_training_testing(model_name, 'ngram_10krows_word2vec_0{}_threshold.pickle'.format(i),'word2vec', train_validation_test, n_layer)\n",
    "        save(grid, 'chart_{}.html'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 85.851437605 training accuracy= 0.5661646586345381 validation accuracy= 0.5124631888935633\n",
      "saving new best validation model - 0.5124631888935633\n",
      "Epoch: 0002 cost= 71.494304478 training accuracy= 0.5732597054886212 validation accuracy= 0.5154606647034077\n",
      "saving new best validation model - 0.5154606647034077\n",
      "Epoch: 0003 cost= 68.367247379 training accuracy= 0.5825635876840696 validation accuracy= 0.5298695835086243\n",
      "saving new best validation model - 0.5298695835086243\n",
      "Epoch: 0004 cost= 67.396349065 training accuracy= 0.5880354752342705 validation accuracy= 0.5378365586874211\n",
      "saving new best validation model - 0.5378365586874211\n",
      "Epoch: 0005 cost= 67.014964361 training accuracy= 0.5924196787148595 validation accuracy= 0.5497212873369793\n",
      "saving new best validation model - 0.5497212873369793\n",
      "Epoch: 0006 cost= 66.809565648 training accuracy= 0.5942938420348058 validation accuracy= 0.557004627681952\n",
      "saving new best validation model - 0.557004627681952\n",
      "Epoch: 0007 cost= 66.678180963 training accuracy= 0.5944946452476573 validation accuracy= 0.5612904922170804\n",
      "saving new best validation model - 0.5612904922170804\n",
      "Epoch: 0008 cost= 66.585913538 training accuracy= 0.5944277108433735 validation accuracy= 0.5659444678165755\n",
      "saving new best validation model - 0.5659444678165755\n",
      "Epoch: 0009 cost= 66.516324489 training accuracy= 0.5947456492637215 validation accuracy= 0.5689682372738746\n",
      "saving new best validation model - 0.5689682372738746\n",
      "Epoch: 0010 cost= 66.463405539 training accuracy= 0.5955153949129853 validation accuracy= 0.5716238956668069\n",
      "saving new best validation model - 0.5716238956668069\n",
      "Epoch: 0011 cost= 66.419866425 training accuracy= 0.5960676037483267 validation accuracy= 0.5748317206562894\n",
      "saving new best validation model - 0.5748317206562894\n",
      "Epoch: 0012 cost= 66.382388232 training accuracy= 0.596268406961178 validation accuracy= 0.5757519983172066\n",
      "saving new best validation model - 0.5757519983172066\n",
      "Epoch: 0013 cost= 66.349468966 training accuracy= 0.5974062918340026 validation accuracy= 0.5770666806899453\n",
      "saving new best validation model - 0.5770666806899453\n",
      "Epoch: 0014 cost= 66.320914967 training accuracy= 0.5980087014725569 validation accuracy= 0.5771192679848549\n",
      "saving new best validation model - 0.5771192679848549\n",
      "Epoch: 0015 cost= 66.296034731 training accuracy= 0.5983266398929049 validation accuracy= 0.5768563315103071\n",
      "reduced validation acc!\n",
      "Epoch: 0016 cost= 66.274012663 training accuracy= 0.5993975903614458 validation accuracy= 0.5772507362221287\n",
      "saving new best validation model - 0.5772507362221287\n",
      "Epoch: 0017 cost= 66.253368045 training accuracy= 0.599313922356091 validation accuracy= 0.5776188472864956\n",
      "saving new best validation model - 0.5776188472864956\n",
      "Epoch: 0018 cost= 66.233448329 training accuracy= 0.5994812583668006 validation accuracy= 0.5775399663441313\n",
      "reduced validation acc!\n",
      "Epoch: 0019 cost= 66.213201353 training accuracy= 0.599966532797858 validation accuracy= 0.578276188472865\n",
      "saving new best validation model - 0.578276188472865\n",
      "Epoch: 0020 cost= 66.193703784 training accuracy= 0.6005354752342704 validation accuracy= 0.5793016407236011\n",
      "saving new best validation model - 0.5793016407236011\n",
      "Epoch: 0021 cost= 66.174340227 training accuracy= 0.6012884872824632 validation accuracy= 0.5796434581405132\n",
      "saving new best validation model - 0.5796434581405132\n",
      "Epoch: 0022 cost= 66.154424504 training accuracy= 0.6013052208835341 validation accuracy= 0.580958140513252\n",
      "saving new best validation model - 0.580958140513252\n",
      "Epoch: 0023 cost= 66.134111612 training accuracy= 0.6014892904953146 validation accuracy= 0.5809055532183425\n",
      "reduced validation acc!\n",
      "Epoch: 0024 cost= 66.113971218 training accuracy= 0.6019076305220884 validation accuracy= 0.5823517038283551\n",
      "saving new best validation model - 0.5823517038283551\n",
      "Epoch: 0025 cost= 66.094117904 training accuracy= 0.6015394912985275 validation accuracy= 0.582824989482541\n",
      "saving new best validation model - 0.582824989482541\n",
      "Epoch: 0026 cost= 66.074410060 training accuracy= 0.6020749665327979 validation accuracy= 0.5831405132519983\n",
      "saving new best validation model - 0.5831405132519983\n",
      "Epoch: 0027 cost= 66.052858145 training accuracy= 0.6018908969210174 validation accuracy= 0.5831142196045436\n",
      "reduced validation acc!\n",
      "Epoch: 0028 cost= 66.030410249 training accuracy= 0.602024765729585 validation accuracy= 0.5840870845603702\n",
      "saving new best validation model - 0.5840870845603702\n",
      "Epoch: 0029 cost= 66.007060947 training accuracy= 0.6022255689424364 validation accuracy= 0.5849021876314683\n",
      "saving new best validation model - 0.5849021876314683\n",
      "Epoch: 0030 cost= 65.982476405 training accuracy= 0.6025602409638554 validation accuracy= 0.5866638620109381\n",
      "saving new best validation model - 0.5866638620109381\n",
      "Epoch: 0031 cost= 65.958299729 training accuracy= 0.6030789825970548 validation accuracy= 0.5885832982751368\n",
      "saving new best validation model - 0.5885832982751368\n",
      "Epoch: 0032 cost= 65.934470121 training accuracy= 0.6028614457831325 validation accuracy= 0.5905290281867901\n",
      "saving new best validation model - 0.5905290281867901\n",
      "Epoch: 0033 cost= 65.910071835 training accuracy= 0.6027443105756358 validation accuracy= 0.5929480437526293\n",
      "saving new best validation model - 0.5929480437526293\n",
      "Epoch: 0034 cost= 65.886320130 training accuracy= 0.6024263721552878 validation accuracy= 0.5961558687421119\n",
      "saving new best validation model - 0.5961558687421119\n",
      "Epoch: 0035 cost= 65.863461486 training accuracy= 0.6028112449799197 validation accuracy= 0.5992059318468658\n",
      "saving new best validation model - 0.5992059318468658\n",
      "Epoch: 0036 cost= 65.841615865 training accuracy= 0.6028614457831325 validation accuracy= 0.6021771140092553\n",
      "saving new best validation model - 0.6021771140092553\n",
      "Epoch: 0037 cost= 65.820386104 training accuracy= 0.6028112449799197 validation accuracy= 0.6048853596970972\n",
      "saving new best validation model - 0.6048853596970972\n",
      "Epoch: 0038 cost= 65.798754611 training accuracy= 0.6028949129852744 validation accuracy= 0.6062263357172907\n",
      "saving new best validation model - 0.6062263357172907\n",
      "Epoch: 0039 cost= 65.778195207 training accuracy= 0.6031459170013387 validation accuracy= 0.6068310896087505\n",
      "saving new best validation model - 0.6068310896087505\n",
      "Epoch: 0040 cost= 65.758886532 training accuracy= 0.6031961178045515 validation accuracy= 0.607356962557846\n",
      "saving new best validation model - 0.607356962557846\n",
      "Epoch: 0041 cost= 65.740872139 training accuracy= 0.6034471218206158 validation accuracy= 0.6086190576356753\n",
      "saving new best validation model - 0.6086190576356753\n",
      "Epoch: 0042 cost= 65.722277994 training accuracy= 0.6035809906291834 validation accuracy= 0.6095393352965923\n",
      "saving new best validation model - 0.6095393352965923\n",
      "Epoch: 0043 cost= 65.702872732 training accuracy= 0.6036813922356091 validation accuracy= 0.6104333193100547\n",
      "saving new best validation model - 0.6104333193100547\n",
      "Epoch: 0044 cost= 65.681060849 training accuracy= 0.6037985274431058 validation accuracy= 0.6118268826251577\n",
      "saving new best validation model - 0.6118268826251577\n",
      "Epoch: 0045 cost= 65.661467445 training accuracy= 0.6041164658634538 validation accuracy= 0.6135359697097181\n",
      "saving new best validation model - 0.6135359697097181\n",
      "Epoch: 0046 cost= 65.644744905 training accuracy= 0.6041834002677376 validation accuracy= 0.6148506520824569\n",
      "saving new best validation model - 0.6148506520824569\n",
      "Epoch: 0047 cost= 65.631077132 training accuracy= 0.6042838018741633 validation accuracy= 0.6164019772822886\n",
      "saving new best validation model - 0.6164019772822886\n",
      "Epoch: 0048 cost= 65.618483666 training accuracy= 0.6047690763052209 validation accuracy= 0.6183214135464872\n",
      "saving new best validation model - 0.6183214135464872\n",
      "Epoch: 0049 cost= 65.606347498 training accuracy= 0.604735609103079 validation accuracy= 0.6194520403870425\n",
      "saving new best validation model - 0.6194520403870425\n",
      "Epoch: 0050 cost= 65.594858211 training accuracy= 0.6046352074966532 validation accuracy= 0.62031973075305\n",
      "saving new best validation model - 0.62031973075305\n",
      "Epoch: 0051 cost= 65.583524407 training accuracy= 0.6048527443105757 validation accuracy= 0.621844762305427\n",
      "saving new best validation model - 0.621844762305427\n",
      "Epoch: 0052 cost= 65.571598769 training accuracy= 0.6048192771084338 validation accuracy= 0.6227913336137989\n",
      "saving new best validation model - 0.6227913336137989\n",
      "Epoch: 0053 cost= 65.558856070 training accuracy= 0.6051037483266399 validation accuracy= 0.6245004206983593\n",
      "saving new best validation model - 0.6245004206983593\n",
      "Epoch: 0054 cost= 65.550836573 training accuracy= 0.605053547523427 validation accuracy= 0.6248422381152714\n",
      "saving new best validation model - 0.6248422381152714\n",
      "Epoch: 0055 cost= 65.545030891 training accuracy= 0.605053547523427 validation accuracy= 0.6255784602440051\n",
      "saving new best validation model - 0.6255784602440051\n",
      "Epoch: 0056 cost= 65.539751094 training accuracy= 0.6051037483266399 validation accuracy= 0.6257888094236432\n",
      "saving new best validation model - 0.6257888094236432\n",
      "Epoch: 0057 cost= 65.534706352 training accuracy= 0.6056224899598394 validation accuracy= 0.6267353807320152\n",
      "saving new best validation model - 0.6267353807320152\n",
      "Epoch: 0058 cost= 65.529411591 training accuracy= 0.6055722891566265 validation accuracy= 0.6266827934371056\n",
      "reduced validation acc!\n",
      "Epoch: 0059 cost= 65.524063698 training accuracy= 0.6056057563587685 validation accuracy= 0.6264987379049222\n",
      "reduced validation acc!\n",
      "Epoch: 0060 cost= 65.518815710 training accuracy= 0.6058065595716198 validation accuracy= 0.6266564997896508\n",
      "reduced validation acc!\n",
      "Epoch: 0061 cost= 65.513697236 training accuracy= 0.6059236947791165 validation accuracy= 0.6270771981489273\n",
      "saving new best validation model - 0.6270771981489273\n",
      "Epoch: 0062 cost= 65.508381951 training accuracy= 0.605773092369478 validation accuracy= 0.6266564997896508\n",
      "reduced validation acc!\n",
      "Epoch: 0063 cost= 65.503294672 training accuracy= 0.6058734939759036 validation accuracy= 0.6267090870845604\n",
      "reduced validation acc!\n",
      "Epoch: 0064 cost= 65.498130108 training accuracy= 0.606091030789826 validation accuracy= 0.6269194362641985\n",
      "reduced validation acc!\n",
      "Epoch: 0065 cost= 65.492887813 training accuracy= 0.606074297188755 validation accuracy= 0.6269983172065628\n",
      "reduced validation acc!\n",
      "Epoch: 0066 cost= 65.487396758 training accuracy= 0.6059404283801875 validation accuracy= 0.6270771981489273\n",
      "reduced validation acc!\n",
      "Epoch: 0067 cost= 65.480949574 training accuracy= 0.6060073627844712 validation accuracy= 0.6271297854438368\n",
      "saving new best validation model - 0.6271297854438368\n",
      "Epoch: 0068 cost= 65.474505918 training accuracy= 0.6060073627844712 validation accuracy= 0.6272086663862011\n",
      "saving new best validation model - 0.6272086663862011\n",
      "Epoch: 0069 cost= 65.468229463 training accuracy= 0.6061244979919679 validation accuracy= 0.6274978965082036\n",
      "saving new best validation model - 0.6274978965082036\n",
      "Epoch: 0070 cost= 65.462719302 training accuracy= 0.6058734939759036 validation accuracy= 0.6287862852334876\n",
      "saving new best validation model - 0.6287862852334876\n",
      "Epoch: 0071 cost= 65.457248905 training accuracy= 0.6059069611780455 validation accuracy= 0.6286548169962137\n",
      "reduced validation acc!\n",
      "Epoch: 0072 cost= 65.451657409 training accuracy= 0.6058232931726908 validation accuracy= 0.628628523348759\n",
      "reduced validation acc!\n",
      "Epoch: 0073 cost= 65.446397548 training accuracy= 0.6058902275769745 validation accuracy= 0.6287599915860328\n",
      "reduced validation acc!\n",
      "Epoch: 0074 cost= 65.440939585 training accuracy= 0.6058567603748327 validation accuracy= 0.6287862852334876\n",
      "reduced validation acc!\n",
      "Epoch: 0075 cost= 65.435516472 training accuracy= 0.6059069611780455 validation accuracy= 0.6286022297013042\n",
      "reduced validation acc!\n",
      "Epoch: 0076 cost= 65.430529404 training accuracy= 0.6058400267737617 validation accuracy= 0.628628523348759\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.20315389]\n",
      " [ 0.3577204 ]\n",
      " [ 0.51697356]\n",
      " ..., \n",
      " [ 0.60918403]\n",
      " [ 0.57577318]\n",
      " [ 0.61112773]]\n",
      "final results: accuracy: 0.610449075433 auc: 0.6567406273 cost: 0.653605284158 epoch: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/bokeh/io.py:433: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warnings.warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/bokeh/io.py:443: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warnings.warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 82.922028316 training accuracy= 0.5880856760374833 validation accuracy= 0.5976020193521245\n",
      "saving new best validation model - 0.5976020193521245\n",
      "Epoch: 0002 cost= 68.867684363 training accuracy= 0.5918674698795181 validation accuracy= 0.6215555321834245\n",
      "saving new best validation model - 0.6215555321834245\n",
      "Epoch: 0003 cost= 67.415122404 training accuracy= 0.5932898259705489 validation accuracy= 0.6348601177955406\n",
      "saving new best validation model - 0.6348601177955406\n",
      "Epoch: 0004 cost= 66.974214238 training accuracy= 0.5937248995983936 validation accuracy= 0.6422749263777872\n",
      "saving new best validation model - 0.6422749263777872\n",
      "Epoch: 0005 cost= 66.690780966 training accuracy= 0.5939591700133868 validation accuracy= 0.6503733697938578\n",
      "saving new best validation model - 0.6503733697938578\n",
      "Epoch: 0006 cost= 66.478370852 training accuracy= 0.5956827309236947 validation accuracy= 0.6579196466133782\n",
      "saving new best validation model - 0.6579196466133782\n",
      "Epoch: 0007 cost= 66.304739736 training accuracy= 0.5978748326639893 validation accuracy= 0.6712242322254943\n",
      "saving new best validation model - 0.6712242322254943\n",
      "Epoch: 0008 cost= 66.170257294 training accuracy= 0.5996151271753681 validation accuracy= 0.6816365166175852\n",
      "saving new best validation model - 0.6816365166175852\n",
      "Epoch: 0009 cost= 66.074939581 training accuracy= 0.6000669344042838 validation accuracy= 0.690891880521666\n",
      "saving new best validation model - 0.690891880521666\n",
      "Epoch: 0010 cost= 65.999933220 training accuracy= 0.601004016064257 validation accuracy= 0.6970971813209929\n",
      "saving new best validation model - 0.6970971813209929\n",
      "Epoch: 0011 cost= 65.937501419 training accuracy= 0.6010207496653279 validation accuracy= 0.703039545645772\n",
      "saving new best validation model - 0.703039545645772\n",
      "Epoch: 0012 cost= 65.881302276 training accuracy= 0.6020080321285141 validation accuracy= 0.7100599495161969\n",
      "saving new best validation model - 0.7100599495161969\n",
      "Epoch: 0013 cost= 65.827864917 training accuracy= 0.6019578313253012 validation accuracy= 0.7145561632309634\n",
      "saving new best validation model - 0.7145561632309634\n",
      "Epoch: 0014 cost= 65.779228856 training accuracy= 0.6022255689424364 validation accuracy= 0.718684265881363\n",
      "saving new best validation model - 0.718684265881363\n",
      "Epoch: 0015 cost= 65.737010400 training accuracy= 0.6028279785809906 validation accuracy= 0.7230753050063105\n",
      "saving new best validation model - 0.7230753050063105\n",
      "Epoch: 0016 cost= 65.699054041 training accuracy= 0.6034136546184738 validation accuracy= 0.7257835506941523\n",
      "saving new best validation model - 0.7257835506941523\n",
      "Epoch: 0017 cost= 65.664109414 training accuracy= 0.603714859437751 validation accuracy= 0.7287021455616323\n",
      "saving new best validation model - 0.7287021455616323\n",
      "Epoch: 0018 cost= 65.633171682 training accuracy= 0.603363453815261 validation accuracy= 0.7319888514934791\n",
      "saving new best validation model - 0.7319888514934791\n",
      "Epoch: 0019 cost= 65.604158756 training accuracy= 0.6035642570281124 validation accuracy= 0.7331457719814892\n",
      "saving new best validation model - 0.7331457719814892\n",
      "Epoch: 0020 cost= 65.578249320 training accuracy= 0.6036311914323963 validation accuracy= 0.7346182162389566\n",
      "saving new best validation model - 0.7346182162389566\n",
      "Epoch: 0021 cost= 65.555466893 training accuracy= 0.6039825970548862 validation accuracy= 0.7367742953302482\n",
      "saving new best validation model - 0.7367742953302482\n",
      "Epoch: 0022 cost= 65.533811323 training accuracy= 0.6040997322623829 validation accuracy= 0.7391144299537232\n",
      "saving new best validation model - 0.7391144299537232\n",
      "Epoch: 0023 cost= 65.515551505 training accuracy= 0.6046352074966532 validation accuracy= 0.7419804375262936\n",
      "saving new best validation model - 0.7419804375262936\n",
      "Epoch: 0024 cost= 65.494502005 training accuracy= 0.6064089692101741 validation accuracy= 0.7446360959192259\n",
      "saving new best validation model - 0.7446360959192259\n",
      "Epoch: 0025 cost= 65.476424225 training accuracy= 0.6066934404283801 validation accuracy= 0.7456352545225073\n",
      "saving new best validation model - 0.7456352545225073\n",
      "Epoch: 0026 cost= 65.459149429 training accuracy= 0.6068440428380187 validation accuracy= 0.7477124526714346\n",
      "saving new best validation model - 0.7477124526714346\n",
      "Epoch: 0027 cost= 65.444542200 training accuracy= 0.6071954484605087 validation accuracy= 0.7478702145561632\n",
      "saving new best validation model - 0.7478702145561632\n",
      "Epoch: 0028 cost= 65.431365705 training accuracy= 0.6075635876840696 validation accuracy= 0.7466344131257888\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 65.420113728 training accuracy= 0.6072958500669344 validation accuracy= 0.7460033655868742\n",
      "reduced validation acc!\n",
      "Epoch: 0030 cost= 65.411417918 training accuracy= 0.6074799196787148 validation accuracy= 0.7462137147665124\n",
      "reduced validation acc!\n",
      "Epoch: 0031 cost= 65.404737706 training accuracy= 0.6073293172690764 validation accuracy= 0.7458718973496004\n",
      "reduced validation acc!\n",
      "Epoch: 0032 cost= 65.398691545 training accuracy= 0.6075635876840696 validation accuracy= 0.7450305006310476\n",
      "reduced validation acc!\n",
      "Epoch: 0033 cost= 65.393184886 training accuracy= 0.6078313253012049 validation accuracy= 0.7451882625157762\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.48954156]\n",
      " [ 0.47495148]\n",
      " [ 0.22961481]\n",
      " ..., \n",
      " [ 0.63068581]\n",
      " [ 0.63018537]\n",
      " [ 0.25490895]]\n",
      "final results: accuracy: 0.612914587614 auc: 0.66003047566 cost: 0.651673484737 epoch: 33\n",
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 4247.060999834 training accuracy= 0.5068775100401607 validation accuracy= 0.6383045856121161\n",
      "saving new best validation model - 0.6383045856121161\n",
      "Epoch: 0002 cost= 334.379266966 training accuracy= 0.55035140562249 validation accuracy= 0.696492427429533\n",
      "saving new best validation model - 0.696492427429533\n",
      "Epoch: 0003 cost= 239.612958196 training accuracy= 0.5561412315930389 validation accuracy= 0.7179743374000841\n",
      "saving new best validation model - 0.7179743374000841\n",
      "Epoch: 0004 cost= 193.749326696 training accuracy= 0.5606927710843373 validation accuracy= 0.7291754312158183\n",
      "saving new best validation model - 0.7291754312158183\n",
      "Epoch: 0005 cost= 162.250106236 training accuracy= 0.5647255689424364 validation accuracy= 0.7373527555742533\n",
      "saving new best validation model - 0.7373527555742533\n",
      "Epoch: 0006 cost= 137.498230095 training accuracy= 0.5660642570281125 validation accuracy= 0.7367742953302482\n",
      "reduced validation acc!\n",
      "Epoch: 0007 cost= 118.680038299 training accuracy= 0.5661813922356091 validation accuracy= 0.735512200252419\n",
      "reduced validation acc!\n",
      "Epoch: 0008 cost= 103.276563878 training accuracy= 0.5673862115127175 validation accuracy= 0.7403765250315524\n",
      "saving new best validation model - 0.7403765250315524\n",
      "Epoch: 0009 cost= 90.930914414 training accuracy= 0.5697456492637215 validation accuracy= 0.748369793857804\n",
      "saving new best validation model - 0.748369793857804\n",
      "Epoch: 0010 cost= 81.675783672 training accuracy= 0.5723560910307898 validation accuracy= 0.7570992848127892\n",
      "saving new best validation model - 0.7570992848127892\n",
      "Epoch: 0011 cost= 75.160451534 training accuracy= 0.574313922356091 validation accuracy= 0.768694783340345\n",
      "saving new best validation model - 0.768694783340345\n",
      "Epoch: 0012 cost= 71.254835982 training accuracy= 0.576690093708166 validation accuracy= 0.7725073622212874\n",
      "saving new best validation model - 0.7725073622212874\n",
      "Epoch: 0013 cost= 69.789194283 training accuracy= 0.5772590361445783 validation accuracy= 0.7756625999158603\n",
      "saving new best validation model - 0.7756625999158603\n",
      "Epoch: 0014 cost= 69.218433533 training accuracy= 0.5785475234270415 validation accuracy= 0.777923853596971\n",
      "saving new best validation model - 0.777923853596971\n",
      "Epoch: 0015 cost= 68.870470466 training accuracy= 0.5810240963855422 validation accuracy= 0.7771876314682373\n",
      "reduced validation acc!\n",
      "Epoch: 0016 cost= 68.553175219 training accuracy= 0.5833835341365462 validation accuracy= 0.7775820361800589\n",
      "reduced validation acc!\n",
      "Epoch: 0017 cost= 68.264814316 training accuracy= 0.5852744310575636 validation accuracy= 0.7788967185527976\n",
      "saving new best validation model - 0.7788967185527976\n",
      "Epoch: 0018 cost= 68.036775343 training accuracy= 0.5872991967871486 validation accuracy= 0.7774242742953302\n",
      "reduced validation acc!\n",
      "Epoch: 0019 cost= 67.857931926 training accuracy= 0.5892737617135207 validation accuracy= 0.7763199411022297\n",
      "reduced validation acc!\n",
      "Epoch: 0020 cost= 67.689181061 training accuracy= 0.5938420348058903 validation accuracy= 0.7717185527976441\n",
      "reduced validation acc!\n",
      "Epoch: 0021 cost= 67.493035104 training accuracy= 0.5976907630522088 validation accuracy= 0.7661705931846866\n",
      "reduced validation acc!\n",
      "Epoch: 0022 cost= 67.275858178 training accuracy= 0.6015394912985275 validation accuracy= 0.761095919225915\n",
      "reduced validation acc!\n",
      "Epoch: 0023 cost= 67.055574280 training accuracy= 0.6037817938420348 validation accuracy= 0.7472128733697938\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.41689432]\n",
      " [ 0.41689432]\n",
      " [ 0.41689432]\n",
      " ..., \n",
      " [ 0.41689432]\n",
      " [ 0.49221665]\n",
      " [ 0.41689432]]\n",
      "final results: accuracy: 0.586909304373 auc: 0.580635734572 cost: 0.68234219071 epoch: 23\n",
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 353.018685806 training accuracy= 0.47031459170013384 validation accuracy= 0.11979385780395456\n",
      "saving new best validation model - 0.11979385780395456\n",
      "Epoch: 0002 cost= 230.924595435 training accuracy= 0.47031459170013384 validation accuracy= 0.11979385780395456\n",
      "reduced validation acc!\n",
      "Epoch: 0003 cost= 173.306491091 training accuracy= 0.47031459170013384 validation accuracy= 0.11979385780395456\n",
      "reduced validation acc!\n",
      "Epoch: 0004 cost= 138.709369768 training accuracy= 0.47100066934404283 validation accuracy= 0.12305427008834666\n",
      "saving new best validation model - 0.12305427008834666\n",
      "Epoch: 0005 cost= 86.415203740 training accuracy= 0.4714190093708166 validation accuracy= 0.12358014303744215\n",
      "saving new best validation model - 0.12358014303744215\n",
      "Epoch: 0006 cost= 70.103617785 training accuracy= 0.5283634538152611 validation accuracy= 0.8720025241901557\n",
      "saving new best validation model - 0.8720025241901557\n",
      "Epoch: 0007 cost= 69.571256091 training accuracy= 0.5289491298527443 validation accuracy= 0.8737904922170804\n",
      "saving new best validation model - 0.8737904922170804\n",
      "Epoch: 0008 cost= 69.384444833 training accuracy= 0.5293507362784471 validation accuracy= 0.8764461506100126\n",
      "saving new best validation model - 0.8764461506100126\n",
      "Epoch: 0009 cost= 69.276986165 training accuracy= 0.5294678714859438 validation accuracy= 0.8778923012200253\n",
      "saving new best validation model - 0.8778923012200253\n",
      "Epoch: 0010 cost= 69.209325653 training accuracy= 0.5297858099062919 validation accuracy= 0.8784707614640302\n",
      "saving new best validation model - 0.8784707614640302\n",
      "Epoch: 0011 cost= 69.183395705 training accuracy= 0.5298360107095047 validation accuracy= 0.8787074042911233\n",
      "saving new best validation model - 0.8787074042911233\n",
      "Epoch: 0012 cost= 69.168323498 training accuracy= 0.5298862115127175 validation accuracy= 0.8790755153554901\n",
      "saving new best validation model - 0.8790755153554901\n",
      "Epoch: 0013 cost= 69.147954037 training accuracy= 0.5299698795180723 validation accuracy= 0.8792332772402187\n",
      "saving new best validation model - 0.8792332772402187\n",
      "Epoch: 0014 cost= 69.146572656 training accuracy= 0.5299531459170014 validation accuracy= 0.8792858645351284\n",
      "saving new best validation model - 0.8792858645351284\n",
      "Epoch: 0015 cost= 69.145331927 training accuracy= 0.5299364123159304 validation accuracy= 0.8793384518300379\n",
      "saving new best validation model - 0.8793384518300379\n",
      "Epoch: 0016 cost= 69.143820068 training accuracy= 0.5300702811244979 validation accuracy= 0.8794699200673117\n",
      "saving new best validation model - 0.8794699200673117\n",
      "Epoch: 0017 cost= 69.142708272 training accuracy= 0.5300702811244979 validation accuracy= 0.879443626419857\n",
      "reduced validation acc!\n",
      "Epoch: 0018 cost= 69.141309703 training accuracy= 0.5300702811244979 validation accuracy= 0.8794962137147665\n",
      "saving new best validation model - 0.8794962137147665\n",
      "Epoch: 0019 cost= 69.140621243 training accuracy= 0.5300702811244979 validation accuracy= 0.879443626419857\n",
      "reduced validation acc!\n",
      "Epoch: 0020 cost= 69.140571045 training accuracy= 0.530087014725569 validation accuracy= 0.8794699200673117\n",
      "reduced validation acc!\n",
      "Epoch: 0021 cost= 69.139840054 training accuracy= 0.5301037483266399 validation accuracy= 0.879548801009676\n",
      "saving new best validation model - 0.879548801009676\n",
      "Epoch: 0022 cost= 69.139246917 training accuracy= 0.5301037483266399 validation accuracy= 0.8795750946571308\n",
      "saving new best validation model - 0.8795750946571308\n",
      "Epoch: 0023 cost= 69.138564424 training accuracy= 0.5301204819277109 validation accuracy= 0.8795225073622213\n",
      "reduced validation acc!\n",
      "Epoch: 0024 cost= 69.139409652 training accuracy= 0.5301037483266399 validation accuracy= 0.8794699200673117\n",
      "reduced validation acc!\n",
      "Epoch: 0025 cost= 69.138579479 training accuracy= 0.5301037483266399 validation accuracy= 0.8794699200673117\n",
      "reduced validation acc!\n",
      "Epoch: 0026 cost= 69.136892782 training accuracy= 0.5301204819277109 validation accuracy= 0.8794173327724022\n",
      "reduced validation acc!\n",
      "Epoch: 0027 cost= 69.135603274 training accuracy= 0.5301372155287818 validation accuracy= 0.8793647454774927\n",
      "reduced validation acc!\n",
      "Epoch: 0028 cost= 69.133778801 training accuracy= 0.5301204819277109 validation accuracy= 0.8794962137147665\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.47447672]\n",
      " [ 0.47447672]\n",
      " [ 0.47447672]\n",
      " ..., \n",
      " [ 0.47447672]\n",
      " [ 0.47447672]\n",
      " [ 0.47447672]]\n",
      "final results: accuracy: 0.529087173466 auc: 0.500331788114 cost: 0.691493673411 epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples - 300195\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 75.172118692 training accuracy= 0.5985276237112543 validation accuracy= 0.5965834077906838\n",
      "saving new best validation model - 0.5965834077906838\n",
      "Epoch: 0002 cost= 67.024654969 training accuracy= 0.6041206549076433 validation accuracy= 0.584101244215664\n",
      "reduced validation acc!\n",
      "Epoch: 0003 cost= 66.346618721 training accuracy= 0.605826212961575 validation accuracy= 0.5800805472948645\n",
      "reduced validation acc!\n",
      "Epoch: 0004 cost= 65.895954899 training accuracy= 0.6088209330601776 validation accuracy= 0.5736194273674103\n",
      "reduced validation acc!\n",
      "Epoch: 0005 cost= 65.532518701 training accuracy= 0.6117290427888539 validation accuracy= 0.5702588448664435\n",
      "reduced validation acc!\n",
      "Epoch: 0006 cost= 65.252282060 training accuracy= 0.6124785556055231 validation accuracy= 0.5672116500193367\n",
      "reduced validation acc!\n",
      "Epoch: 0007 cost= 65.055188416 training accuracy= 0.6140575292726395 validation accuracy= 0.5629509114913251\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.492495  ]\n",
      " [ 0.58607709]\n",
      " [ 0.43806818]\n",
      " ..., \n",
      " [ 0.58959955]\n",
      " [ 0.5764941 ]\n",
      " [ 0.49056393]]\n",
      "final results: accuracy: 0.593537594029 auc: 0.605430143761 cost: 0.678948740334 epoch: 7\n",
      "number of samples - 300195\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 136.144006596 training accuracy= 0.6003464414797048 validation accuracy= 0.5113819728752984\n",
      "saving new best validation model - 0.5113819728752984\n",
      "Epoch: 0002 cost= 68.500311852 training accuracy= 0.6028847915521578 validation accuracy= 0.5482283595823276\n",
      "saving new best validation model - 0.5482283595823276\n",
      "Epoch: 0003 cost= 67.022506369 training accuracy= 0.6056396675494262 validation accuracy= 0.5546361369304013\n",
      "saving new best validation model - 0.5546361369304013\n",
      "Epoch: 0004 cost= 66.339419269 training accuracy= 0.6086710304968437 validation accuracy= 0.5602037686532332\n",
      "saving new best validation model - 0.5602037686532332\n",
      "Epoch: 0005 cost= 65.889645772 training accuracy= 0.6118423025033728 validation accuracy= 0.5623441396508728\n",
      "saving new best validation model - 0.5623441396508728\n",
      "Epoch: 0006 cost= 65.598551467 training accuracy= 0.6120988024450774 validation accuracy= 0.562117433688506\n",
      "reduced validation acc!\n",
      "Epoch: 0007 cost= 65.435227570 training accuracy= 0.6121954063192259 validation accuracy= 0.5618107138570686\n",
      "reduced validation acc!\n",
      "Epoch: 0008 cost= 65.306598269 training accuracy= 0.6134978930361932 validation accuracy= 0.5625375065011269\n",
      "saving new best validation model - 0.5625375065011269\n",
      "Epoch: 0009 cost= 65.192815634 training accuracy= 0.6140042305834541 validation accuracy= 0.5606171736434316\n",
      "reduced validation acc!\n",
      "Epoch: 0010 cost= 65.096330085 training accuracy= 0.613844334515898 validation accuracy= 0.5596036646352034\n",
      "reduced validation acc!\n",
      "Epoch: 0011 cost= 65.021769350 training accuracy= 0.6133646463132297 validation accuracy= 0.5623641431181404\n",
      "reduced validation acc!\n",
      "Epoch: 0012 cost= 64.958482009 training accuracy= 0.6134312696747114 validation accuracy= 0.5630976035846214\n",
      "saving new best validation model - 0.5630976035846214\n",
      "Epoch: 0013 cost= 64.903173150 training accuracy= 0.6142840487016772 validation accuracy= 0.5636977076026511\n",
      "saving new best validation model - 0.5636977076026511\n",
      "Epoch: 0014 cost= 64.851538615 training accuracy= 0.6147204317193824 validation accuracy= 0.5632909704348754\n",
      "reduced validation acc!\n",
      "Epoch: 0015 cost= 64.789172344 training accuracy= 0.6173920285147987 validation accuracy= 0.5644578393588222\n",
      "saving new best validation model - 0.5644578393588222\n",
      "Epoch: 0016 cost= 64.690854982 training accuracy= 0.6175918985992438 validation accuracy= 0.5653646632082895\n",
      "saving new best validation model - 0.5653646632082895\n",
      "Epoch: 0017 cost= 64.586522783 training accuracy= 0.6180915738103566 validation accuracy= 0.5654513448997827\n",
      "saving new best validation model - 0.5654513448997827\n",
      "Epoch: 0018 cost= 64.554121500 training accuracy= 0.6181648595079865 validation accuracy= 0.5659180924693613\n",
      "saving new best validation model - 0.5659180924693613\n",
      "Epoch: 0019 cost= 64.527541971 training accuracy= 0.6183680607605057 validation accuracy= 0.56607145238508\n",
      "saving new best validation model - 0.56607145238508\n",
      "Epoch: 0020 cost= 64.506435539 training accuracy= 0.6186412165425806 validation accuracy= 0.5661648018989959\n",
      "saving new best validation model - 0.5661648018989959\n",
      "Epoch: 0021 cost= 64.485216022 training accuracy= 0.61900764503073 validation accuracy= 0.5666315494685745\n",
      "saving new best validation model - 0.5666315494685745\n",
      "Epoch: 0022 cost= 64.468832406 training accuracy= 0.6190409567114709 validation accuracy= 0.5682584981396775\n",
      "saving new best validation model - 0.5682584981396775\n",
      "Epoch: 0023 cost= 64.456942473 training accuracy= 0.6191475540898416 validation accuracy= 0.5691586541667222\n",
      "saving new best validation model - 0.5691586541667222\n",
      "Epoch: 0024 cost= 64.446252479 training accuracy= 0.6191475540898416 validation accuracy= 0.569372024484244\n",
      "saving new best validation model - 0.569372024484244\n",
      "Epoch: 0025 cost= 64.437489314 training accuracy= 0.6191075800729526 validation accuracy= 0.5693986957739341\n",
      "saving new best validation model - 0.5693986957739341\n",
      "Epoch: 0026 cost= 64.430601995 training accuracy= 0.6192008527790269 validation accuracy= 0.5696053982690333\n",
      "saving new best validation model - 0.5696053982690333\n",
      "Epoch: 0027 cost= 64.423775370 training accuracy= 0.619164209930212 validation accuracy= 0.569191993278835\n",
      "reduced validation acc!\n",
      "Epoch: 0028 cost= 64.416032511 training accuracy= 0.6190742683922117 validation accuracy= 0.5689119447370877\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 64.403023107 training accuracy= 0.6188277619547294 validation accuracy= 0.5684785362796217\n",
      "reduced validation acc!\n",
      "Epoch: 0030 cost= 64.390552467 training accuracy= 0.6185979113576175 validation accuracy= 0.5674583594489712\n",
      "reduced validation acc!\n",
      "Epoch: 0031 cost= 64.380422116 training accuracy= 0.6180382751211713 validation accuracy= 0.5668382519636737\n",
      "reduced validation acc!\n",
      "Epoch: 0032 cost= 64.373909654 training accuracy= 0.6181182231549492 validation accuracy= 0.5662581514129116\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.62829566]\n",
      " [ 0.53092682]\n",
      " [ 0.45543456]\n",
      " ..., \n",
      " [ 0.59850383]\n",
      " [ 0.60743558]\n",
      " [ 0.56879747]]\n",
      "final results: accuracy: 0.606628528644 auc: 0.62372629592 cost: 0.650630655629 epoch: 32\n",
      "number of samples - 300195\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 415.374715293 training accuracy= 0.5565282566331884 validation accuracy= 0.2989584861375972\n",
      "saving new best validation model - 0.2989584861375972\n",
      "Epoch: 0002 cost= 67.822882889 training accuracy= 0.555219107580073 validation accuracy= 0.29535786202941844\n",
      "reduced validation acc!\n",
      "Epoch: 0003 cost= 67.385711644 training accuracy= 0.5563650293975583 validation accuracy= 0.2964513849067172\n",
      "reduced validation acc!\n",
      "Epoch: 0004 cost= 67.153764917 training accuracy= 0.5590432885291228 validation accuracy= 0.30129889180791336\n",
      "saving new best validation model - 0.30129889180791336\n",
      "Epoch: 0005 cost= 66.894159925 training accuracy= 0.5640267159679542 validation accuracy= 0.31129395761932066\n",
      "saving new best validation model - 0.31129395761932066\n",
      "Epoch: 0006 cost= 66.567504374 training accuracy= 0.5708189676710138 validation accuracy= 0.32824356221745105\n",
      "saving new best validation model - 0.32824356221745105\n",
      "Epoch: 0007 cost= 66.243588756 training accuracy= 0.5765019404054031 validation accuracy= 0.34598663768386523\n",
      "saving new best validation model - 0.34598663768386523\n",
      "Epoch: 0008 cost= 66.051336906 training accuracy= 0.5784706607371874 validation accuracy= 0.35199434568658566\n",
      "saving new best validation model - 0.35199434568658566\n",
      "Epoch: 0009 cost= 65.974189374 training accuracy= 0.5807058745148986 validation accuracy= 0.36152266392841426\n",
      "saving new best validation model - 0.36152266392841426\n",
      "Epoch: 0010 cost= 65.925991684 training accuracy= 0.5811988873898633 validation accuracy= 0.3628828997026151\n",
      "saving new best validation model - 0.3628828997026151\n",
      "Epoch: 0011 cost= 65.889536980 training accuracy= 0.5824514065857193 validation accuracy= 0.3686438982757011\n",
      "saving new best validation model - 0.3686438982757011\n",
      "Epoch: 0012 cost= 65.857027904 training accuracy= 0.5829544129649061 validation accuracy= 0.37082427620787606\n",
      "saving new best validation model - 0.37082427620787606\n",
      "Epoch: 0013 cost= 65.831586608 training accuracy= 0.5838038608237979 validation accuracy= 0.3747249523250697\n",
      "saving new best validation model - 0.3747249523250697\n",
      "Epoch: 0014 cost= 65.797688086 training accuracy= 0.5881310481520345 validation accuracy= 0.3912011415311987\n",
      "saving new best validation model - 0.3912011415311987\n",
      "Epoch: 0015 cost= 65.718449183 training accuracy= 0.5907993137793768 validation accuracy= 0.40073612759544985\n",
      "saving new best validation model - 0.40073612759544985\n",
      "Epoch: 0016 cost= 65.646159180 training accuracy= 0.5942237545595364 validation accuracy= 0.4126315227972849\n",
      "saving new best validation model - 0.4126315227972849\n",
      "Epoch: 0017 cost= 65.577389714 training accuracy= 0.5976581888439181 validation accuracy= 0.4252603784656007\n",
      "saving new best validation model - 0.4252603784656007\n",
      "Epoch: 0018 cost= 65.505643290 training accuracy= 0.5991172404603674 validation accuracy= 0.430241241815248\n",
      "saving new best validation model - 0.430241241815248\n",
      "Epoch: 0019 cost= 65.458022905 training accuracy= 0.5996535585202951 validation accuracy= 0.4313080934028565\n",
      "saving new best validation model - 0.4313080934028565\n",
      "Epoch: 0020 cost= 65.417690039 training accuracy= 0.6003497726477789 validation accuracy= 0.43490871751103527\n",
      "saving new best validation model - 0.43490871751103527\n",
      "Epoch: 0021 cost= 65.387804876 training accuracy= 0.600706207631706 validation accuracy= 0.43604891514529187\n",
      "saving new best validation model - 0.43604891514529187\n",
      "Epoch: 0022 cost= 65.355526181 training accuracy= 0.6009493829011143 validation accuracy= 0.4350487417819089\n",
      "reduced validation acc!\n",
      "Epoch: 0023 cost= 65.325176086 training accuracy= 0.6019454021552657 validation accuracy= 0.4428634296611413\n",
      "saving new best validation model - 0.4428634296611413\n",
      "Epoch: 0024 cost= 65.288190549 training accuracy= 0.6025550059128233 validation accuracy= 0.4484910717857762\n",
      "saving new best validation model - 0.4484910717857762\n",
      "Epoch: 0025 cost= 65.249038555 training accuracy= 0.6036109861923084 validation accuracy= 0.4542387347140171\n",
      "saving new best validation model - 0.4542387347140171\n",
      "Epoch: 0026 cost= 65.204923909 training accuracy= 0.604930128749646 validation accuracy= 0.46240681718164484\n",
      "saving new best validation model - 0.46240681718164484\n",
      "Epoch: 0027 cost= 65.127394138 training accuracy= 0.6068655374006896 validation accuracy= 0.46833451131529463\n",
      "saving new best validation model - 0.46833451131529463\n",
      "Epoch: 0028 cost= 65.055016093 training accuracy= 0.6077949332933593 validation accuracy= 0.47650259378292237\n",
      "saving new best validation model - 0.47650259378292237\n",
      "Epoch: 0029 cost= 64.907598769 training accuracy= 0.6123319842102634 validation accuracy= 0.5008134743355515\n",
      "saving new best validation model - 0.5008134743355515\n",
      "Epoch: 0030 cost= 64.775535755 training accuracy= 0.6138576591881943 validation accuracy= 0.5042607385280116\n",
      "saving new best validation model - 0.5042607385280116\n",
      "Epoch: 0031 cost= 64.664117281 training accuracy= 0.6146571395259748 validation accuracy= 0.5135423473402057\n",
      "saving new best validation model - 0.5135423473402057\n",
      "Epoch: 0032 cost= 64.532457763 training accuracy= 0.615996269091757 validation accuracy= 0.5183031725499087\n",
      "saving new best validation model - 0.5183031725499087\n",
      "Epoch: 0033 cost= 64.412073273 training accuracy= 0.6178250803644297 validation accuracy= 0.5268179817835091\n",
      "saving new best validation model - 0.5268179817835091\n",
      "Epoch: 0034 cost= 64.317591702 training accuracy= 0.6191342294175453 validation accuracy= 0.5298185018736581\n",
      "saving new best validation model - 0.5298185018736581\n",
      "Epoch: 0035 cost= 64.257029151 training accuracy= 0.619164209930212 validation accuracy= 0.538580020536893\n",
      "saving new best validation model - 0.538580020536893\n",
      "Epoch: 0036 cost= 64.214083913 training accuracy= 0.6189143723246556 validation accuracy= 0.5382933041727233\n",
      "reduced validation acc!\n",
      "Epoch: 0037 cost= 64.180618342 training accuracy= 0.6185879178533953 validation accuracy= 0.5389667542374011\n",
      "saving new best validation model - 0.5389667542374011\n",
      "Epoch: 0038 cost= 64.151553401 training accuracy= 0.6182114958610236 validation accuracy= 0.5410471148332378\n",
      "saving new best validation model - 0.5410471148332378\n",
      "Epoch: 0039 cost= 64.119428073 training accuracy= 0.6181048984826529 validation accuracy= 0.5402869830770667\n",
      "reduced validation acc!\n",
      "Epoch: 0040 cost= 64.086454770 training accuracy= 0.618574593181099 validation accuracy= 0.5487217784415965\n",
      "saving new best validation model - 0.5487217784415965\n",
      "Epoch: 0041 cost= 64.052941370 training accuracy= 0.6191109112410267 validation accuracy= 0.5531158734180591\n",
      "saving new best validation model - 0.5531158734180591\n",
      "Epoch: 0042 cost= 64.025834081 training accuracy= 0.6187511450890255 validation accuracy= 0.5549295211169936\n",
      "saving new best validation model - 0.5549295211169936\n",
      "Epoch: 0043 cost= 64.004684503 training accuracy= 0.6193674111827312 validation accuracy= 0.5522557243255498\n",
      "reduced validation acc!\n",
      "Epoch: 0044 cost= 63.986714902 training accuracy= 0.6197638201835474 validation accuracy= 0.54758158080734\n",
      "reduced validation acc!\n",
      "Epoch: 0045 cost= 63.973090618 training accuracy= 0.6197038591582138 validation accuracy= 0.5499419899449238\n",
      "reduced validation acc!\n",
      "Epoch: 0046 cost= 63.959622544 training accuracy= 0.6198104565365845 validation accuracy= 0.5509221598410391\n",
      "reduced validation acc!\n",
      "Epoch: 0047 cost= 63.948294720 training accuracy= 0.6199770149402888 validation accuracy= 0.5557563310973902\n",
      "saving new best validation model - 0.5557563310973902\n",
      "Epoch: 0048 cost= 63.937149167 training accuracy= 0.620063625310215 validation accuracy= 0.5545294517716404\n",
      "reduced validation acc!\n",
      "Epoch: 0049 cost= 63.924440011 training accuracy= 0.6202901447392528 validation accuracy= 0.5602637790550362\n",
      "saving new best validation model - 0.5602637790550362\n",
      "Epoch: 0050 cost= 63.911760685 training accuracy= 0.6208231316311065 validation accuracy= 0.5588368650566098\n",
      "reduced validation acc!\n",
      "Epoch: 0051 cost= 63.900693668 training accuracy= 0.6208098069588102 validation accuracy= 0.5538560017069626\n",
      "reduced validation acc!\n",
      "Epoch: 0052 cost= 63.890713179 training accuracy= 0.6211329302619963 validation accuracy= 0.5650046007974716\n",
      "saving new best validation model - 0.5650046007974716\n",
      "Epoch: 0053 cost= 63.886670326 training accuracy= 0.6210596445643665 validation accuracy= 0.563804392761412\n",
      "reduced validation acc!\n",
      "Epoch: 0054 cost= 63.873060771 training accuracy= 0.6212628458168857 validation accuracy= 0.5761732033552482\n",
      "saving new best validation model - 0.5761732033552482\n",
      "Epoch: 0055 cost= 63.859079724 training accuracy= 0.6213627808591082 validation accuracy= 0.5704588795391201\n",
      "reduced validation acc!\n",
      "Epoch: 0056 cost= 63.849797395 training accuracy= 0.6217392028514799 validation accuracy= 0.5676117193646899\n",
      "reduced validation acc!\n",
      "Epoch: 0057 cost= 63.840588188 training accuracy= 0.6216792418261463 validation accuracy= 0.5665982103564617\n",
      "reduced validation acc!\n",
      "Epoch: 0058 cost= 63.826356698 training accuracy= 0.6220523326504439 validation accuracy= 0.5657847360209103\n",
      "reduced validation acc!\n",
      "Epoch: 0059 cost= 63.812039004 training accuracy= 0.6225986442145939 validation accuracy= 0.5667982450291383\n",
      "reduced validation acc!\n",
      "Epoch: 0060 cost= 63.796283560 training accuracy= 0.6225620013657789 validation accuracy= 0.5682718337845226\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.62756562]\n",
      " [ 0.62756562]\n",
      " [ 0.5242905 ]\n",
      " ..., \n",
      " [ 0.38165814]\n",
      " [ 0.41441262]\n",
      " [ 0.62756562]]\n",
      "final results: accuracy: 0.612302436856 auc: 0.642992656065 cost: 0.641052965129 epoch: 60\n",
      "number of samples - 300195\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 233.342884223 training accuracy= 0.530931561151918 validation accuracy= 0.2541507194580394\n",
      "saving new best validation model - 0.2541507194580394\n",
      "Epoch: 0002 cost= 69.275181637 training accuracy= 0.5368577091557154 validation accuracy= 0.28234227266059453\n",
      "saving new best validation model - 0.28234227266059453\n",
      "Epoch: 0003 cost= 68.901022027 training accuracy= 0.5437898699178867 validation accuracy= 0.3289970261511995\n",
      "saving new best validation model - 0.3289970261511995\n",
      "Epoch: 0004 cost= 68.247298258 training accuracy= 0.603167940838455 validation accuracy= 0.5277714803899343\n",
      "saving new best validation model - 0.5277714803899343\n",
      "Epoch: 0005 cost= 67.114876110 training accuracy= 0.5991205716284416 validation accuracy= 0.52535772867297\n",
      "reduced validation acc!\n",
      "Epoch: 0006 cost= 66.480675421 training accuracy= 0.5985309548793284 validation accuracy= 0.49866643551548934\n",
      "reduced validation acc!\n",
      "Epoch: 0007 cost= 65.797094098 training accuracy= 0.6012458568597079 validation accuracy= 0.4899649272540574\n",
      "reduced validation acc!\n",
      "Epoch: 0008 cost= 65.361817906 training accuracy= 0.6047335898332751 validation accuracy= 0.5014135783535812\n",
      "reduced validation acc!\n",
      "Epoch: 0009 cost= 65.208536932 training accuracy= 0.6055264078349073 validation accuracy= 0.5027338071932468\n",
      "reduced validation acc!\n",
      "Epoch: 0010 cost= 65.114166555 training accuracy= 0.6067422841819484 validation accuracy= 0.5097283529145051\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.500723  ]\n",
      " [ 0.43887436]\n",
      " [ 0.57501727]\n",
      " ..., \n",
      " [ 0.58713508]\n",
      " [ 0.58713508]\n",
      " [ 0.48502657]]\n",
      "final results: accuracy: 0.594847857368 auc: 0.601342639042 cost: 0.677755738983 epoch: 10\n",
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 845785.303025544 training accuracy= 0.5947958500669344 validation accuracy= 0.6403291964661337\n",
      "saving new best validation model - 0.6403291964661337\n",
      "Epoch: 0002 cost= 536592.711159967 training accuracy= 0.654718875502008 validation accuracy= 0.7039861169541439\n",
      "saving new best validation model - 0.7039861169541439\n",
      "Epoch: 0003 cost= 410374.265939070 training accuracy= 0.6853246318607764 validation accuracy= 0.7379838031131679\n",
      "saving new best validation model - 0.7379838031131679\n",
      "Epoch: 0004 cost= 341749.939515285 training accuracy= 0.7015060240963855 validation accuracy= 0.7577566259991586\n",
      "saving new best validation model - 0.7577566259991586\n",
      "Epoch: 0005 cost= 296809.259683836 training accuracy= 0.7130689424364123 validation accuracy= 0.7686421960454354\n",
      "saving new best validation model - 0.7686421960454354\n",
      "Epoch: 0006 cost= 263901.740970477 training accuracy= 0.7215361445783133 validation accuracy= 0.7786600757257046\n",
      "saving new best validation model - 0.7786600757257046\n",
      "Epoch: 0007 cost= 238037.089222152 training accuracy= 0.7276773761713521 validation accuracy= 0.7836558687421119\n",
      "saving new best validation model - 0.7836558687421119\n",
      "Epoch: 0008 cost= 216911.287701528 training accuracy= 0.7319611780455154 validation accuracy= 0.7871529238535969\n",
      "saving new best validation model - 0.7871529238535969\n",
      "Epoch: 0009 cost= 199166.377473304 training accuracy= 0.7361947791164659 validation accuracy= 0.7882046697517879\n",
      "saving new best validation model - 0.7882046697517879\n",
      "Epoch: 0010 cost= 183955.069579669 training accuracy= 0.7394578313253012 validation accuracy= 0.7885464871687\n",
      "saving new best validation model - 0.7885464871687\n",
      "Epoch: 0011 cost= 170506.060131386 training accuracy= 0.7419511378848729 validation accuracy= 0.7915965502734539\n",
      "saving new best validation model - 0.7915965502734539\n",
      "Epoch: 0012 cost= 158564.290606679 training accuracy= 0.7444779116465864 validation accuracy= 0.7931215818258309\n",
      "saving new best validation model - 0.7931215818258309\n",
      "Epoch: 0013 cost= 147707.348232046 training accuracy= 0.7468038821954485 validation accuracy= 0.7952250736222128\n",
      "saving new best validation model - 0.7952250736222128\n",
      "Epoch: 0014 cost= 137869.194193624 training accuracy= 0.7488119143239625 validation accuracy= 0.7985117795540597\n",
      "saving new best validation model - 0.7985117795540597\n",
      "Epoch: 0015 cost= 128928.329714981 training accuracy= 0.7501171352074967 validation accuracy= 0.8012726125368111\n",
      "saving new best validation model - 0.8012726125368111\n",
      "Epoch: 0016 cost= 120614.399968593 training accuracy= 0.7521251673360108 validation accuracy= 0.8016670172486328\n",
      "saving new best validation model - 0.8016670172486328\n",
      "Epoch: 0017 cost= 112951.334622330 training accuracy= 0.7536479250334672 validation accuracy= 0.8037705090450147\n",
      "saving new best validation model - 0.8037705090450147\n",
      "Epoch: 0018 cost= 105852.998449278 training accuracy= 0.756408969210174 validation accuracy= 0.8025084139671855\n",
      "reduced validation acc!\n",
      "Epoch: 0019 cost= 99226.551059333 training accuracy= 0.7583668005354752 validation accuracy= 0.8019825410180901\n",
      "reduced validation acc!\n",
      "Epoch: 0020 cost= 93065.461912427 training accuracy= 0.7604250334672021 validation accuracy= 0.8019562473706352\n",
      "reduced validation acc!\n",
      "Epoch: 0021 cost= 87304.549861940 training accuracy= 0.7624163319946452 validation accuracy= 0.802324358435002\n",
      "reduced validation acc!\n",
      "Epoch: 0022 cost= 81877.826780386 training accuracy= 0.7647088353413655 validation accuracy= 0.8029291123264619\n",
      "reduced validation acc!\n",
      "Epoch: 0023 cost= 76802.103214641 training accuracy= 0.7659638554216868 validation accuracy= 0.8067942785023139\n",
      "saving new best validation model - 0.8067942785023139\n",
      "Epoch: 0024 cost= 72039.381889918 training accuracy= 0.7683065595716199 validation accuracy= 0.8029554059739167\n",
      "reduced validation acc!\n",
      "Epoch: 0025 cost= 67505.526967520 training accuracy= 0.7692938420348059 validation accuracy= 0.804086032814472\n",
      "reduced validation acc!\n",
      "Epoch: 0026 cost= 63242.646886778 training accuracy= 0.7699631860776439 validation accuracy= 0.8062158182583088\n",
      "reduced validation acc!\n",
      "Epoch: 0027 cost= 59202.529928287 training accuracy= 0.7720548862115127 validation accuracy= 0.8055847707193942\n",
      "reduced validation acc!\n",
      "Epoch: 0028 cost= 55345.772707941 training accuracy= 0.772958500669344 validation accuracy= 0.8049537231804796\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 51672.853373312 training accuracy= 0.775033467202142 validation accuracy= 0.8052166596550273\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "final results: accuracy: 0.749104784268 auc: 0.748397207973 cost: 891.309715292 epoch: 29\n",
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 187620910.365159124 training accuracy= 0.6089859437751004 validation accuracy= 0.6572623054270088\n",
      "saving new best validation model - 0.6572623054270088\n",
      "Epoch: 0002 cost= 118342359.216080397 training accuracy= 0.660475234270415 validation accuracy= 0.7093763146823727\n",
      "saving new best validation model - 0.7093763146823727\n",
      "Epoch: 0003 cost= 91743580.134003356 training accuracy= 0.6855421686746987 validation accuracy= 0.7365902397980648\n",
      "saving new best validation model - 0.7365902397980648\n",
      "Epoch: 0004 cost= 76360639.376884416 training accuracy= 0.7007028112449799 validation accuracy= 0.7531026503996634\n",
      "saving new best validation model - 0.7531026503996634\n",
      "Epoch: 0005 cost= 65651015.678391963 training accuracy= 0.7095883534136547 validation accuracy= 0.7630679427850231\n",
      "saving new best validation model - 0.7630679427850231\n",
      "Epoch: 0006 cost= 57591531.390284754 training accuracy= 0.7174698795180723 validation accuracy= 0.7687736642827093\n",
      "saving new best validation model - 0.7687736642827093\n",
      "Epoch: 0007 cost= 51172536.623115577 training accuracy= 0.7241131191432396 validation accuracy= 0.7725599495161969\n",
      "saving new best validation model - 0.7725599495161969\n",
      "Epoch: 0008 cost= 45877714.881072029 training accuracy= 0.7297021419009371 validation accuracy= 0.7812631468237274\n",
      "saving new best validation model - 0.7812631468237274\n",
      "Epoch: 0009 cost= 41419842.214405358 training accuracy= 0.7343540829986613 validation accuracy= 0.7849442574673958\n",
      "saving new best validation model - 0.7849442574673958\n",
      "Epoch: 0010 cost= 37541864.460636519 training accuracy= 0.7378346720214191 validation accuracy= 0.791228439209087\n",
      "saving new best validation model - 0.791228439209087\n",
      "Epoch: 0011 cost= 34137686.723618090 training accuracy= 0.742536813922356 validation accuracy= 0.7917017248632731\n",
      "saving new best validation model - 0.7917017248632731\n",
      "Epoch: 0012 cost= 31121234.624790620 training accuracy= 0.7466867469879518 validation accuracy= 0.7939366848969289\n",
      "saving new best validation model - 0.7939366848969289\n",
      "Epoch: 0013 cost= 28447457.162479062 training accuracy= 0.750987282463186 validation accuracy= 0.7947780816154817\n",
      "saving new best validation model - 0.7947780816154817\n",
      "Epoch: 0014 cost= 26068265.490787271 training accuracy= 0.7545348058902276 validation accuracy= 0.7957246529238536\n",
      "saving new best validation model - 0.7957246529238536\n",
      "Epoch: 0015 cost= 23905481.120603014 training accuracy= 0.7572121820615797 validation accuracy= 0.7968026924694994\n",
      "saving new best validation model - 0.7968026924694994\n",
      "Epoch: 0016 cost= 21954735.490787271 training accuracy= 0.761813922356091 validation accuracy= 0.7970130416491376\n",
      "saving new best validation model - 0.7970130416491376\n",
      "Epoch: 0017 cost= 20173844.198492464 training accuracy= 0.7648929049531459 validation accuracy= 0.7986169541438788\n",
      "saving new best validation model - 0.7986169541438788\n",
      "Epoch: 0018 cost= 18558343.538525961 training accuracy= 0.768607764390897 validation accuracy= 0.7980910811947833\n",
      "reduced validation acc!\n",
      "Epoch: 0019 cost= 17083185.264656618 training accuracy= 0.7718875502008032 validation accuracy= 0.7963557004627682\n",
      "reduced validation acc!\n",
      "Epoch: 0020 cost= 15721434.030988274 training accuracy= 0.7758199464524765 validation accuracy= 0.791938367690366\n",
      "reduced validation acc!\n",
      "Epoch: 0021 cost= 14476363.296482412 training accuracy= 0.777727576974565 validation accuracy= 0.7951461926798485\n",
      "reduced validation acc!\n",
      "Epoch: 0022 cost= 13324315.700586265 training accuracy= 0.77975234270415 validation accuracy= 0.7979596129575095\n",
      "reduced validation acc!\n",
      "Epoch: 0023 cost= 12267787.914991625 training accuracy= 0.781425702811245 validation accuracy= 0.7985643668489693\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "final results: accuracy: 0.74018197828 auc: 0.738583119955 cost: 253304.027708 epoch: 23\n",
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 33068011941.949748993 training accuracy= 0.6194946452476573 validation accuracy= 0.6647034076567101\n",
      "saving new best validation model - 0.6647034076567101\n",
      "Epoch: 0002 cost= 19329623893.333332062 training accuracy= 0.6620314591700134 validation accuracy= 0.7074831720656289\n",
      "saving new best validation model - 0.7074831720656289\n",
      "Epoch: 0003 cost= 14521593473.500837326 training accuracy= 0.6832161981258367 validation accuracy= 0.7284917963819941\n",
      "saving new best validation model - 0.7284917963819941\n",
      "Epoch: 0004 cost= 11833757916.408710480 training accuracy= 0.6975903614457831 validation accuracy= 0.7415334455195625\n",
      "saving new best validation model - 0.7415334455195625\n",
      "Epoch: 0005 cost= 9972870912.857622147 training accuracy= 0.706777108433735 validation accuracy= 0.7484486748001683\n",
      "saving new best validation model - 0.7484486748001683\n",
      "Epoch: 0006 cost= 8555136924.087101936 training accuracy= 0.7144912985274431 validation accuracy= 0.7473706352545225\n",
      "reduced validation acc!\n",
      "Epoch: 0007 cost= 7422587769.353433609 training accuracy= 0.720615796519411 validation accuracy= 0.7562053007993269\n",
      "saving new best validation model - 0.7562053007993269\n",
      "Epoch: 0008 cost= 6494018153.487437248 training accuracy= 0.7276773761713521 validation accuracy= 0.7616217921750105\n",
      "saving new best validation model - 0.7616217921750105\n",
      "Epoch: 0009 cost= 5721165220.234505653 training accuracy= 0.7343875502008033 validation accuracy= 0.7691680689945309\n",
      "saving new best validation model - 0.7691680689945309\n",
      "Epoch: 0010 cost= 5069873672.361808777 training accuracy= 0.7388888888888889 validation accuracy= 0.7778975599495161\n",
      "saving new best validation model - 0.7778975599495161\n",
      "Epoch: 0011 cost= 4517584250.211054802 training accuracy= 0.7454986613119143 validation accuracy= 0.7782393773664282\n",
      "saving new best validation model - 0.7782393773664282\n",
      "Epoch: 0012 cost= 4039034478.847571373 training accuracy= 0.748644578313253 validation accuracy= 0.7817627261253681\n",
      "saving new best validation model - 0.7817627261253681\n",
      "Epoch: 0013 cost= 3621713524.422110558 training accuracy= 0.7529618473895582 validation accuracy= 0.7851283129995793\n",
      "saving new best validation model - 0.7851283129995793\n",
      "Epoch: 0014 cost= 3253420499.189279556 training accuracy= 0.7570113788487283 validation accuracy= 0.7856541859486748\n",
      "saving new best validation model - 0.7856541859486748\n",
      "Epoch: 0015 cost= 2930319667.135678291 training accuracy= 0.7623661311914324 validation accuracy= 0.7826304164913757\n",
      "reduced validation acc!\n",
      "Epoch: 0016 cost= 2644928633.889447212 training accuracy= 0.7654953145917002 validation accuracy= 0.7855753050063105\n",
      "reduced validation acc!\n",
      "Epoch: 0017 cost= 2393505333.922947884 training accuracy= 0.7688420348058902 validation accuracy= 0.7871792175010518\n",
      "saving new best validation model - 0.7871792175010518\n",
      "Epoch: 0018 cost= 2171917859.591289997 training accuracy= 0.7720883534136547 validation accuracy= 0.7885464871687\n",
      "saving new best validation model - 0.7885464871687\n",
      "Epoch: 0019 cost= 1973660918.673366785 training accuracy= 0.7726740294511378 validation accuracy= 0.794856962557846\n",
      "saving new best validation model - 0.794856962557846\n",
      "Epoch: 0020 cost= 1797332620.810720205 training accuracy= 0.7752844712182062 validation accuracy= 0.7965397559949516\n",
      "saving new best validation model - 0.7965397559949516\n",
      "Epoch: 0021 cost= 1639679008.804020166 training accuracy= 0.777392904953146 validation accuracy= 0.7991428270929744\n",
      "saving new best validation model - 0.7991428270929744\n",
      "Epoch: 0022 cost= 1496896628.743718624 training accuracy= 0.7822121820615796 validation accuracy= 0.797670382835507\n",
      "reduced validation acc!\n",
      "Epoch: 0023 cost= 1368245265.956448793 training accuracy= 0.7868808567603748 validation accuracy= 0.795093605384939\n",
      "reduced validation acc!\n",
      "Epoch: 0024 cost= 1252181386.023450613 training accuracy= 0.7912985274431058 validation accuracy= 0.7922013041649137\n",
      "reduced validation acc!\n",
      "Epoch: 0025 cost= 1149209111.530988216 training accuracy= 0.7888554216867469 validation accuracy= 0.8006678586453513\n",
      "saving new best validation model - 0.8006678586453513\n",
      "Epoch: 0026 cost= 1057535426.867671728 training accuracy= 0.7866633199464524 validation accuracy= 0.806189524610854\n",
      "saving new best validation model - 0.806189524610854\n",
      "Epoch: 0027 cost= 977155858.010050297 training accuracy= 0.7811579651941097 validation accuracy= 0.8148401346234749\n",
      "saving new best validation model - 0.8148401346234749\n",
      "Epoch: 0028 cost= 899602834.331658244 training accuracy= 0.7888554216867469 validation accuracy= 0.8115797223390829\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 830339118.901172519 training accuracy= 0.7874497991967871 validation accuracy= 0.8128681110643668\n",
      "reduced validation acc!\n",
      "Epoch: 0030 cost= 767488944.160804033 training accuracy= 0.802058232931727 validation accuracy= 0.8033235170382835\n",
      "reduced validation acc!\n",
      "Epoch: 0031 cost= 711417310.874371886 training accuracy= 0.8048862115127176 validation accuracy= 0.8041123264619268\n",
      "reduced validation acc!\n",
      "Epoch: 0032 cost= 661611603.886097193 training accuracy= 0.8104919678714859 validation accuracy= 0.800352334875894\n",
      "reduced validation acc!\n",
      "Epoch: 0033 cost= 616996568.777219415 training accuracy= 0.793239625167336 validation accuracy= 0.816470340765671\n",
      "saving new best validation model - 0.816470340765671\n",
      "Epoch: 0034 cost= 581362069.628140688 training accuracy= 0.7873661311914324 validation accuracy= 0.8231226335717291\n",
      "saving new best validation model - 0.8231226335717291\n",
      "Epoch: 0035 cost= 541510458.318257928 training accuracy= 0.797991967871486 validation accuracy= 0.8187053007993269\n",
      "reduced validation acc!\n",
      "Epoch: 0036 cost= 508213553.366834164 training accuracy= 0.8201974564926372 validation accuracy= 0.8042437946992007\n",
      "reduced validation acc!\n",
      "Epoch: 0037 cost= 472177968.335008383 training accuracy= 0.8234939759036145 validation accuracy= 0.8012200252419015\n",
      "reduced validation acc!\n",
      "Epoch: 0038 cost= 443450732.636515915 training accuracy= 0.8279283801874163 validation accuracy= 0.7996161127471603\n",
      "reduced validation acc!\n",
      "Epoch: 0039 cost= 419239978.599664986 training accuracy= 0.8305388219544846 validation accuracy= 0.7848390828775768\n",
      "reduced validation acc!\n",
      "Epoch: 0040 cost= 397371820.140703499 training accuracy= 0.818908969210174 validation accuracy= 0.7650136726966765\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "final results: accuracy: 0.734487819196 auc: 0.730064078766 cost: 13870860.7887 epoch: 40\n",
      "number of samples - 59760\n",
      "running for maximum of 1000 epochs or validation failed 6 times\n",
      "Epoch: 0001 cost= 4275231421822.499023438 training accuracy= 0.6088186077643909 validation accuracy= 0.6573937736642828\n",
      "saving new best validation model - 0.6573937736642828\n",
      "Epoch: 0002 cost= 2447921697090.465820312 training accuracy= 0.6438420348058902 validation accuracy= 0.6964398401346235\n",
      "saving new best validation model - 0.6964398401346235\n",
      "Epoch: 0003 cost= 1805452954653.159179688 training accuracy= 0.6630187416331995 validation accuracy= 0.7083508624316365\n",
      "saving new best validation model - 0.7083508624316365\n",
      "Epoch: 0004 cost= 1424686509784.978271484 training accuracy= 0.6764725568942437 validation accuracy= 0.7261516617585192\n",
      "saving new best validation model - 0.7261516617585192\n",
      "Epoch: 0005 cost= 1163079083095.477294922 training accuracy= 0.6873326639892905 validation accuracy= 0.7363535969709718\n",
      "saving new best validation model - 0.7363535969709718\n",
      "Epoch: 0006 cost= 968263539514.747070312 training accuracy= 0.6980589022757697 validation accuracy= 0.7357488430795119\n",
      "reduced validation acc!\n",
      "Epoch: 0007 cost= 816090233041.259643555 training accuracy= 0.704066265060241 validation accuracy= 0.7434791754312158\n",
      "saving new best validation model - 0.7434791754312158\n",
      "Epoch: 0008 cost= 694434076620.542724609 training accuracy= 0.7102074966532798 validation accuracy= 0.7503681110643668\n",
      "saving new best validation model - 0.7503681110643668\n",
      "Epoch: 0009 cost= 595133857517.561157227 training accuracy= 0.7159638554216867 validation accuracy= 0.7531289440471182\n",
      "saving new best validation model - 0.7531289440471182\n",
      "Epoch: 0010 cost= 512713914165.601318359 training accuracy= 0.7227409638554216 validation accuracy= 0.7535759360538494\n",
      "saving new best validation model - 0.7535759360538494\n",
      "Epoch: 0011 cost= 443925986449.795654297 training accuracy= 0.7292336010709505 validation accuracy= 0.7485538493899874\n",
      "reduced validation acc!\n",
      "Epoch: 0012 cost= 386005661081.943054199 training accuracy= 0.7342034805890227 validation accuracy= 0.7511043331931005\n",
      "reduced validation acc!\n",
      "Epoch: 0013 cost= 337019304200.147399902 training accuracy= 0.7376506024096385 validation accuracy= 0.7479753891459824\n",
      "reduced validation acc!\n",
      "Epoch: 0014 cost= 295513344449.393615723 training accuracy= 0.742904953145917 validation accuracy= 0.7502629364745478\n",
      "reduced validation acc!\n",
      "Epoch: 0015 cost= 260063535886.150756836 training accuracy= 0.7482095046854083 validation accuracy= 0.753497055111485\n",
      "reduced validation acc!\n",
      "Epoch: 0016 cost= 230292143414.458953857 training accuracy= 0.7522255689424364 validation accuracy= 0.7618847286495583\n",
      "saving new best validation model - 0.7618847286495583\n",
      "Epoch: 0017 cost= 204383707671.155792236 training accuracy= 0.7563755020080322 validation accuracy= 0.7642774505679428\n",
      "saving new best validation model - 0.7642774505679428\n",
      "Epoch: 0018 cost= 182229754459.765502930 training accuracy= 0.7602576974564926 validation accuracy= 0.7569152292806058\n",
      "reduced validation acc!\n",
      "Epoch: 0019 cost= 162753293162.773864746 training accuracy= 0.7647255689424364 validation accuracy= 0.7556794278502313\n",
      "reduced validation acc!\n",
      "Epoch: 0020 cost= 145904070429.587951660 training accuracy= 0.7679551539491298 validation accuracy= 0.7700094657130837\n",
      "saving new best validation model - 0.7700094657130837\n",
      "Epoch: 0021 cost= 130879037426.278060913 training accuracy= 0.7728580990629184 validation accuracy= 0.761095919225915\n",
      "reduced validation acc!\n",
      "Epoch: 0022 cost= 118012007586.948074341 training accuracy= 0.775686077643909 validation accuracy= 0.767406394615061\n",
      "reduced validation acc!\n",
      "Epoch: 0023 cost= 106742405953.608047485 training accuracy= 0.780053547523427 validation accuracy= 0.7581510307109802\n",
      "reduced validation acc!\n",
      "Epoch: 0024 cost= 96783235487.088775635 training accuracy= 0.7839524765729585 validation accuracy= 0.7598864114429954\n",
      "reduced validation acc!\n",
      "Epoch: 0025 cost= 88302794233.996643066 training accuracy= 0.7873995983935743 validation accuracy= 0.7736905763567522\n",
      "saving new best validation model - 0.7736905763567522\n",
      "Epoch: 0026 cost= 81046816215.691787720 training accuracy= 0.7923360107095047 validation accuracy= 0.7663809423643247\n",
      "reduced validation acc!\n",
      "Epoch: 0027 cost= 75158924426.934677124 training accuracy= 0.7947958500669344 validation accuracy= 0.7562315944467817\n",
      "reduced validation acc!\n",
      "Epoch: 0028 cost= 69478198729.969848633 training accuracy= 0.8008366800535476 validation accuracy= 0.771955195624737\n",
      "reduced validation acc!\n",
      "Epoch: 0029 cost= 64594491337.112228394 training accuracy= 0.7870983935742972 validation accuracy= 0.8014829617164493\n",
      "saving new best validation model - 0.8014829617164493\n",
      "Epoch: 0030 cost= 59840573721.299835205 training accuracy= 0.7688755020080321 validation accuracy= 0.8148664282709297\n",
      "saving new best validation model - 0.8148664282709297\n",
      "Epoch: 0031 cost= 55983499013.574539185 training accuracy= 0.7763888888888889 validation accuracy= 0.8131836348338242\n",
      "reduced validation acc!\n",
      "Epoch: 0032 cost= 52559413110.780570984 training accuracy= 0.7759036144578313 validation accuracy= 0.8155500631047539\n",
      "saving new best validation model - 0.8155500631047539\n",
      "Epoch: 0033 cost= 48990969142.458961487 training accuracy= 0.7833835341365462 validation accuracy= 0.8122896508203618\n",
      "reduced validation acc!\n",
      "Epoch: 0034 cost= 46124518648.710220337 training accuracy= 0.7926539491298528 validation accuracy= 0.8086611274716029\n",
      "reduced validation acc!\n",
      "Epoch: 0035 cost= 43402910697.701843262 training accuracy= 0.802359437751004 validation accuracy= 0.8042700883466555\n",
      "reduced validation acc!\n",
      "Epoch: 0036 cost= 40779726403.752090454 training accuracy= 0.803012048192771 validation accuracy= 0.8076356752208667\n",
      "reduced validation acc!\n",
      "Epoch: 0037 cost= 38373007167.892799377 training accuracy= 0.82464859437751 validation accuracy= 0.792280185107278\n",
      "reduced validation acc!\n",
      "Epoch: 0038 cost= 36981658632.576217651 training accuracy= 0.8352074966532798 validation accuracy= 0.7707982751367269\n",
      "reduced validation acc!\n",
      "Optimization Finished!\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "final results: accuracy: 0.712943938949 auc: 0.706969642833 cost: 1391186251.09 epoch: 38\n"
     ]
    }
   ],
   "source": [
    "train_validation_test = split_test_validation('ngram_10krows_ml_kn.pickle')\n",
    "for i in ['ngram_10krows_ml_kn.pickle','ngram_10krows_tfidf.pickle','ngram_10krows_word2vec_05_threshold.pickle']:\n",
    "    for n_layer in [1,2,3,4]:\n",
    "        epoch_values=[]\n",
    "        cost_values=[]\n",
    "        training_accuracy_values=[]\n",
    "        validation_accuracy_values=[]\n",
    "        \n",
    "        update(epoch_values,cost_values,training_accuracy_values,validation_accuracy_values)\n",
    "        model_name = '{}_layers{}'.format(i.replace('.pickle',''), n_layer)\n",
    "        \n",
    "        run_training_testing(model_name, i, re.search('ngram_10krows_([^_\\.]*)[_\\.]',i).groups(1)[0], train_validation_test, n_layer)\n",
    "        save(grid, 'chart_{}.html'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "mnist.train.labels\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load_word2vec_format('/media/sf_vmSharedFolder/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "model.similarity('king','queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML(smoothing) vs TFIDF vs Word2Vec , know the limit\n",
    "### run till convergence\n",
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_features, train_labels.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64104818853323953"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_features, test_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
