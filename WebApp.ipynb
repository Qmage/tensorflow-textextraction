{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "def create_architecture(n_input, layers):\n",
    "\n",
    "    hidden_features = {\n",
    "        10:[7,5,3,2],\n",
    "        1000:[666,444,300,200]\n",
    "    }\n",
    "\n",
    "    n_hidden_1 = hidden_features[n_input][0] # 1st layer number of features\n",
    "    n_hidden_2 = hidden_features[n_input][1] # 2nd layer number of features\n",
    "    n_hidden_3 = hidden_features[n_input][2] # 3rd layer number of features\n",
    "    n_hidden_4 = hidden_features[n_input][3] # 3rd layer number of features\n",
    "    n_classes = 1 # total classes (binary)\n",
    "\n",
    "    # Create model\n",
    "    def multilayer_perceptron(layers, x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        layer_4 = tf.nn.relu(layer_4)\n",
    "            \n",
    "        #Output layer with linear activation\n",
    "        out_layer = {\n",
    "            1: tf.matmul(layer_1, weights['out1']) + biases['out'],\n",
    "            2: tf.matmul(layer_2, weights['out2']) + biases['out'],\n",
    "            3: tf.matmul(layer_3, weights['out3']) + biases['out'],\n",
    "            4: tf.matmul(layer_4, weights['out4']) + biases['out']\n",
    "                     }\n",
    "        return out_layer[layers]\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input,n_hidden_1],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_input+\n",
    "                                                             n_hidden_1+1)),\n",
    "                                           name=\"h1\")),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_1+\n",
    "                                                             n_hidden_2+1)),\n",
    "                                           name=\"h2\")),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_2,n_hidden_3],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_2+\n",
    "                                                             n_hidden_3+1)),\n",
    "                                           name=\"h3\")),\n",
    "        'h4': tf.Variable(tf.random_normal([n_hidden_3,n_hidden_4],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_3+\n",
    "                                                             n_hidden_4+1)),\n",
    "                                           name=\"h4\")),\n",
    "        'out1': tf.Variable(tf.random_normal([n_hidden_1,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_1+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\")),\n",
    "        'out2': tf.Variable(tf.random_normal([n_hidden_2,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_2+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\")),\n",
    "        'out3': tf.Variable(tf.random_normal([n_hidden_3,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_3+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\")),\n",
    "        'out4': tf.Variable(tf.random_normal([n_hidden_4,n_classes],\n",
    "                                           mean=0,\n",
    "                                           stddev=(np.sqrt(6/n_hidden_4+\n",
    "                                                             n_classes+1)),\n",
    "                                           name=\"out\"))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_1+1)),\n",
    "                                        name=\"b1\")),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_2+1)),\n",
    "                                        name=\"b2\")),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_3],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_3+1)),\n",
    "                                        name=\"b3\")),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_4],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_hidden_4+1)),\n",
    "                                        name=\"b4\")),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes],\n",
    "                                        mean=0,\n",
    "                                        stddev=(np.sqrt(6/n_classes+1)),\n",
    "                                        name=\"biasout\"))\n",
    "    }\n",
    "    \n",
    "    # Parameters\n",
    "    learning_rate = 0.001 # the alpha\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(layers, x, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    #cost = tf.nn.l2_loss(pred-y, name=\"squared_error_cost\")\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    predict_op = tf.nn.sigmoid(pred)\n",
    "    \n",
    "    return x, y, cost, optimizer, predict_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "nltk.download(info_or_id='stopwords')\n",
    "nltk.download(info_or_id='punkt')\n",
    "sw = set(stopwords.words(\"english\")) | set(i.strip() for i in list(open('stopwordlist.txt'))[1:])\n",
    "terms_df = pd.read_csv('finalterms_grouped_clean2.csv')\n",
    "single_terms = set(b for a in terms_df['term'] for b in nltk.word_tokenize(a) if (b not in sw and len(b)> 1))\n",
    "del stopwords\n",
    "del terms_df\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "puncs = set(string.punctuation)\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "def return_words_from_text(x):\n",
    "    def return_tokens(fulltext,start,end):\n",
    "        text = fulltext[start:end]\n",
    "        return tuple( [\n",
    "            tuple(t.strip(string.punctuation) for t in tokenizer.tokenize(re.sub(r'\\d+','SOMENUM',text.lower()))), \n",
    "            tuple((t[0]+start,t[1]+start) for t in tokenizer.span_tokenize(text))\n",
    "                ]  )\n",
    "    return tuple(return_tokens(x.strip(), sent[0], sent[1]) for sent in sent_detector.span_tokenize(x.strip()))\n",
    "\n",
    "def get_ngrams(x, ngram_start, ngram_end):\n",
    "    def get_ngram(sent, i):\n",
    "        span = [(s[0][0],s[-1][-1]) for s in ngrams(sent[1], i)]\n",
    "        the_ngrams = list(ngrams(sent[0], i))\n",
    "        #print(len(span))\n",
    "        #print(len(the_ngrams))\n",
    "        if len(the_ngrams) > 0:\n",
    "            return zip(the_ngrams, span)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def remove_punc(ngram):\n",
    "        return tuple([tuple(gram for gram in ngram[0] if not all(j in puncs for j in gram)), ngram[1]])\n",
    "    def check_num_only(ngram):\n",
    "        #print(ngram)\n",
    "        return set(ngram[0]) != set('SOMENUM')\n",
    "          \n",
    "    ngram_list = defaultdict(list)\n",
    "    for sent in x:\n",
    "        #print(sent)\n",
    "        for i in range(ngram_start,ngram_end+1):\n",
    "            for gram in map(remove_punc, get_ngram(sent, i)):\n",
    "                if ~check_num_only(gram) and len(gram) >0:\n",
    "                    ngram_list[gram[0]].append(gram[1])\n",
    "    return ngram_list\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"space_100features_5minwords_10context_300karticles.bin\")\n",
    "allwords = set(model.index2word)\n",
    "def get_vector(term):\n",
    "    vector = [model[unigram.lower()] if unigram.lower() in allwords else np.zeros(100) for unigram in term ]\n",
    "    return np.concatenate(vector + [np.zeros(100)]*(10-len(vector)), axis=0)\n",
    "\n",
    "myrake = rake.Rake('stopwordlist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:9999/ (Press CTRL+C to quit)\n",
      "183.171.170.220 - - [31/Oct/2016 03:23:30] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183.171.170.220 - - [31/Oct/2016 03:24:08] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183.171.170.220 - - [31/Oct/2016 03:25:23] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import render_template\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template(\"my_form.html\", result=\"\")\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def my_form_post():\n",
    "    new_text = request.form['text'].strip()\n",
    "    \n",
    "    potential_kw = myrake.run(new_text)\n",
    "    rake_kw = [kw[0] for kw in potential_kw if kw[1] >1]\n",
    "    \n",
    "    ngram_of_article = get_ngrams(return_words_from_text(new_text), 1, 10)\n",
    "    tf.reset_default_graph()\n",
    "    x, y, cost, optimizer, predict_op = create_architecture(1000, 3)\n",
    "    saver = tf.train.Saver()\n",
    "    classify_func = np.vectorize(lambda x: 1 if x >=0.5 else 0)\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"bestmodel_10_new.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "        for i in ngram_of_article:\n",
    "            vector = np.array([0.000001 if y==0 else y for y in get_vector(i) ])\n",
    "            predicts = sess.run(predict_op, feed_dict = {x: [vector]})\n",
    "            ngram_of_article[i] = (ngram_of_article[i], vector, classify_func(predicts)[0][0])\n",
    "            \n",
    "    bold_range = np.zeros(len(new_text))\n",
    "    bold_range.shape\n",
    "    for gram in ngram_of_article:\n",
    "        if ngram_of_article[gram][2]==1 and len(gram)<5:\n",
    "            #print(gram)\n",
    "            for span in ngram_of_article[gram][0]:\n",
    "                bold_range[span[0]:span[1]] = 1\n",
    "    len(bold_range)\n",
    "    sum(bold_range)\n",
    "\n",
    "    html_string = \"\"\"<br><h4>Business Variable from proposed algorithm highlighted in dark red:</h4>\n",
    "    <br>\"\"\"\n",
    "    temp_bold = ''\n",
    "    for idx,i in enumerate(bold_range):\n",
    "        if i==1:\n",
    "            temp_bold += new_text[idx]\n",
    "        else:\n",
    "            if temp_bold:\n",
    "                html_string += '<b style=\"color:DarkRed\">{}</b>'.format(temp_bold)\n",
    "                temp_bold = \"\"\n",
    "            html_string += new_text[idx]\n",
    "    if temp_bold:\n",
    "        html_string += '<b style=\"color:DarkRed\">{}</b>'.format(temp_bold)\n",
    "        temp_bold = \"\"\n",
    "    #print(html_string)\n",
    "    rake_list_html = \"\"\"<h4>Results from conventional keyword extraction algorithm - [Rapid Automatic Keyword Extraction, RAKE] (Rose et. al. 2010):</h4>\n",
    "    <br><ul>{}</ul>\"\"\".format(\"\".join([\"<li>{}</li>\".format(kw) for kw in rake_kw]))\n",
    "    \n",
    "    return render_template(\"my_form.html\", result=html_string, rakeresult=rake_list_html)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0',port=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = myrake.run(\"\"\"\n",
    "Officials from OPEC and non-member oil producing countries met on Saturday aiming to build support for an OPEC plan to reduce output one day after OPEC members were unable to agreed on how to implement the deal.\n",
    "\n",
    "Arriving for the meeting with OPEC's High Level Committee of exporters, only the representative of non-OPEC Azerbaijan made comments supportive of the need for producer action to help prop up prices.\n",
    "\n",
    "\"Today we will discuss the recognized positions of countries, first of all the OPEC countries,\" Azerbaijan's energy minister Natig Aliyev told reporters outside OPEC's headquarters.\n",
    "\n",
    "\"Just one week ago we met with the president of Venezuela,\" he added, in reference to the south American OPEC member which has been pushing for measures to support prices.\n",
    "\n",
    "\"Venezuela and Azerbaijan agree that some measures will be taken to stabilize the market. We agreed the price of oil can be around $60 per barrel.\"\n",
    "\n",
    "Oil LCOc1 is trading closer to $50 a barrel, less than half its price of mid-2014, weighed down by persistent oversupply and squeezing the incomes of exporting nations.\n",
    "\n",
    "Other non-OPEC officials did not mention joint producer action.\n",
    "\n",
    "The deputy minister for Kazakhstan, asked what he hoped the meeting would achieve, said: \"We just hope the price will react and it will increase.\"\n",
    "\n",
    "Brazil's representative said his country was attending only as an observer.\n",
    "\n",
    "\"Brazilian production will increase in the next few years,\" said Brazilian official Marcio Felix.\n",
    "\n",
    "Russia, which is one of the world's top producers and has been supporting action with OPEC to prop up prices, is also attending the meeting, so far without making public comment in Vienna.\n",
    "\n",
    "Two OPEC sources said Russian energy officials told the gathering that Moscow was still willing to freeze its output levels if OPEC agreed to cap its production.\n",
    "\n",
    "\"Russia is ready but they want to see in detail figures agreed for yesterday,\" one of the sources said. Another source said Russia would freeze if OPEC agreed to reduce output.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['energy minister natig aliyev told reporters',\n",
       " 'non-opec azerbaijan made comments supportive',\n",
       " 'non-member oil producing countries met',\n",
       " 'russian energy officials told',\n",
       " 'brazilian official marcio felix',\n",
       " 'south american opec member',\n",
       " 'mention joint producer action',\n",
       " 'high level committee',\n",
       " 'making public comment',\n",
       " 'detail figures agreed',\n",
       " 'deputy minister',\n",
       " 'non-opec officials',\n",
       " 'producer action',\n",
       " 'azerbaijan agree',\n",
       " 'oil lcoc1',\n",
       " 'supporting action',\n",
       " 'brazilian production',\n",
       " 'opec countries',\n",
       " 'build support',\n",
       " 'saturday aiming',\n",
       " 'exporting nations',\n",
       " 'reduce output',\n",
       " 'trading closer',\n",
       " 'top producers',\n",
       " 'recognized positions',\n",
       " 'output levels',\n",
       " 'persistent oversupply',\n",
       " 'week ago',\n",
       " 'opec plan',\n",
       " 'opec members',\n",
       " 'opec agreed',\n",
       " 'support prices',\n",
       " 'opec sources',\n",
       " 'met',\n",
       " 'countries',\n",
       " 'azerbaijan',\n",
       " 'oil',\n",
       " 'officials',\n",
       " 'opec',\n",
       " 'agreed',\n",
       " 'sources',\n",
       " 'production',\n",
       " 'prices']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[kw[0] for kw in output if kw[1] >1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
